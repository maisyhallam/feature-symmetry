{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45d44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # for writing dataframes to csv\n",
    "import random # for making a random choice\n",
    "import os # for scanning directories\n",
    "import itertools\n",
    "import string # for generating strings\n",
    "from collections import Counter\n",
    "\n",
    "import kintypes as kt # bringing large lists of kin types into the namespace\n",
    "import math # for calculating logs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statistics\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# testing = True # set to True to run code blocks with tests and examples\n",
    "# filtering = False # set to True to run the filtering process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d39313",
   "metadata": {},
   "source": [
    "# Internal co-selection\n",
    "\n",
    "Internal co-selection refers to a process of kin term evolution whereby terminological changes in one part of the paradigm co-occur with changes in related parts of the paradigm, increasing the predictive structure of the paradigm.\n",
    "\n",
    "In this notebook, we will build simulations to investigate the robustness of this tendency cross-linguistically, using data from Kinbank, a global database of kin terminology. \n",
    "\n",
    "We will measure internal co-selection in terms of the **mutual information** between Generation N and Generation N+1 in a particular kinship system. That tells us how much information can be gained from one generation by observing the other - how certain we can be about which children 'go with' which parents. This can be calculated as the **entropy** of one generation (how much unpredictable variation there is) minus the **conditional entropy** between the two generations (how much unpredictability remains in one generation after observing another).\n",
    "\n",
    "## Calculating mutual information of a kinship system\n",
    "\n",
    "To calculate the mutual information (MI) of a particular kinship system, we must perform the following steps:\n",
    "\n",
    "1. Extract kin terminology data from Kinbank for this language.\n",
    "2. Condense the full kinship system down to the terms we are interested in: Ego's generation and Ego's parents' generation.\n",
    "3. Calculate the probabilities of each kin term within the generation in which it belongs; and the probabilities of each parent-child pair.\n",
    "4. Calculate entropy, conditional entropy, subtract them from each other to get the mutual information of the system.\n",
    "\n",
    "After we get that going, we can do these same calculations on simulated kinship systems.\n",
    "\n",
    "### 1. Extract kin terminology from Kinbank\n",
    "\n",
    "First, let's actually load our data in. The following function `get_kb_files()` pulls the full list of Kinbank filenames - one file per language. Later, we can iterate through these to generate MI values for every language in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246b31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kb_files(path) -> list:\n",
    "    files = []\n",
    "    directory = os.scandir(path)\n",
    "    morgan = []\n",
    "    for file in directory:\n",
    "        if 'Morgan' in file.name:\n",
    "            morgan.append(file.name)\n",
    "        else:\n",
    "            files.append(file.name)\n",
    "    files += morgan # all morgan files at the end, so if there is a duplicate, the non morgan data is used\n",
    "    return files\n",
    "\n",
    "all_kb_files = get_kb_files('../../kinbank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca211494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ngombe_Binja_binj1250.csv',\n",
       " 'Ottowa_Ojibwa_otta1242.csv',\n",
       " 'Tlingit_tlin1245.csv',\n",
       " 'Bannock_bann1248.csv',\n",
       " 'Sa_saaa1241.csv',\n",
       " 'Kimaama_kima1246.csv',\n",
       " 'Dhuwal-Dhuwala_(Yolngu)_dhuw1248.csv',\n",
       " 'Tenharim_tenh1241.csv',\n",
       " 'Mosetén_Chimané_mose1249.csv',\n",
       " 'Tsaangi_tsaa1242.csv',\n",
       " 'Ngoni_ngon1269.csv',\n",
       " 'Nyakyusa_nyak1260.csv',\n",
       " 'Balaesang_bala1314.csv',\n",
       " 'KazymKhanty_nort2672.csv',\n",
       " 'Aklanon_akla1241.csv',\n",
       " 'Paakantyi_darl1243.csv',\n",
       " 'Fipa_fipa1238.csv',\n",
       " 'Namakura_nama1268b.csv',\n",
       " 'Woleaian_wole1240.csv',\n",
       " 'Dondo_dond1249.csv',\n",
       " 'Nengone_neng1238.csv',\n",
       " 'Punu_punu1239.csv',\n",
       " 'Kuot_kuot1243.csv',\n",
       " 'Buin_buin1247.csv',\n",
       " 'Ontong_Java_onto1237.csv',\n",
       " 'Nunamiut_nort2944.csv',\n",
       " 'Angaite_anga1316.csv',\n",
       " 'Kemtuik_kemt1242.csv',\n",
       " 'Futunans_east2447.csv',\n",
       " 'Iraya_iray1237.csv',\n",
       " 'Mblafe_mbla1238.csv',\n",
       " 'Godoberi_ghod1238.csv',\n",
       " 'Chamarro_cham1312.csv',\n",
       " 'Pipil_pipi1250.csv',\n",
       " 'Malgana_malg1242a.csv',\n",
       " 'Central_Tagbanwa_cent2090.csv',\n",
       " 'Kashmiri_kash1277.csv',\n",
       " 'Macushi_macu1259.csv',\n",
       " 'Haya_haya1250.csv',\n",
       " 'Zigula_zigu1242.csv',\n",
       " 'Bemba_bemb1257.csv',\n",
       " 'Komi_Zyrian_komi1268.csv',\n",
       " 'Mairasi_nucl1594.csv',\n",
       " 'Ekonda_Mongo_ekon1238.csv',\n",
       " 'Pama_(Paamanese)_paam1238.csv',\n",
       " 'Ilokano_ilok1237.csv',\n",
       " 'Tok_Pisin_tokp1240.csv',\n",
       " 'Tsonga_tson1249.csv',\n",
       " 'Modern_Welsh_wels1247.csv',\n",
       " 'Sa_a_saaa1240.csv',\n",
       " 'Dakaka_daka1243.csv',\n",
       " 'Zinza_zinz1238.csv',\n",
       " 'Nimboran_nucl1633.csv',\n",
       " 'Buru_(Indonesia)_buru1303.csv',\n",
       " 'Lipan_Apache_lipa1241.csv',\n",
       " 'Cocopa_coco1261.csv',\n",
       " 'Kawaiisu_kawa1283.csv',\n",
       " 'Kamano_kama1370.csv',\n",
       " 'Navajo_nava1243.csv',\n",
       " 'Tigak_tiga1245.csv',\n",
       " 'Achuar_achu1249.csv',\n",
       " 'Berik_beri1254.csv',\n",
       " 'Southwest_Tanna_sout2869.csv',\n",
       " 'Slovakian_slov1269.csv',\n",
       " 'Bwaidoga_bwai1243.csv',\n",
       " 'Maybrat_maib1239.csv',\n",
       " 'Korafe_kora1295.csv',\n",
       " 'Kankanaey_(Kankanai)_kank1243.csv',\n",
       " 'Lonwolwol_lonw1238.csv',\n",
       " 'Marra_mara1385.csv',\n",
       " 'Maranao_(Lanao_Moro)_mara1404.csv',\n",
       " 'Polish_poli1260.csv',\n",
       " 'Yir-Yoront_yiry1245.csv',\n",
       " 'Lamahalot_lama1277.csv',\n",
       " 'Bilibil_bilb1241.csv',\n",
       " 'Wangkangurru_wang1290.csv',\n",
       " 'Bungku_bung1269.csv',\n",
       " 'Colloquial_Malay_mala1479.csv',\n",
       " 'Tapirapé_tapi1254.csv',\n",
       " 'Dehu_dehu1237.csv',\n",
       " 'Wiwirano_wiwi1237.csv',\n",
       " 'Sirionó_siri1273.csv',\n",
       " 'Futuna-Aniwa_futu1245.csv',\n",
       " 'Wangkangurru_wang1290a.csv',\n",
       " 'Ampibabo_Lauje_ampi1237.csv',\n",
       " 'Belarusian_bela1254.csv',\n",
       " 'Kodava_koda1255.csv',\n",
       " 'Kemiju_(Jate)_keya1238.csv',\n",
       " 'Burmese_nucl1310.csv',\n",
       " 'Negerhollands_nege1244.csv',\n",
       " 'Maguindanao_(Magindonao_Moro)_magu1243.csv',\n",
       " 'Penrhyn_(Tongareva)_penr1237.csv',\n",
       " 'Dharumbal_dhar1248a.csv',\n",
       " 'Yalic_yali1257.csv',\n",
       " 'Sungwaloge_(Nalemba_Edward)_mari1426g.csv',\n",
       " 'Central_Melanau_cent2101.csv',\n",
       " 'Japanese_nucl1643.csv',\n",
       " 'Ayoreo_ayor1240.csv',\n",
       " 'Yadhaykenu_yadh1237.csv',\n",
       " 'Birhor_birh1242.csv',\n",
       " 'Balangao_bala1310.csv',\n",
       " 'Lebanese_Arabic_stan1323.csv',\n",
       " 'Old_Russian_oldr1238.csv',\n",
       " 'Mansi_mans1258.csv',\n",
       " 'Old_Church_Slavonic_chur1257.csv',\n",
       " 'Tolaki_tola1247.csv',\n",
       " 'Yabem_yabe1254.csv',\n",
       " 'Balinese_bali1278.csv',\n",
       " 'Lenakel_lena1238.csv',\n",
       " 'Yami_Tao__yami1254.csv',\n",
       " 'Iwam_iwam1256.csv',\n",
       " 'Pinji_pinj1243.csv',\n",
       " 'Merlav_merl1237.csv',\n",
       " 'Dalabon_ngal1292.csv',\n",
       " 'Kagulu_kagu1239.csv',\n",
       " 'Serbocroation_sout1528.csv',\n",
       " 'Warlpiri_warl1254.csv',\n",
       " 'Kayabí_kaya1329.csv',\n",
       " 'Duna_duna1248.csv',\n",
       " 'Highland_Puebla_Nahuatl_high1278.csv',\n",
       " 'Māori_maor1246.csv',\n",
       " 'Isnag_isna1241.csv',\n",
       " 'Nyulnyul_nyul1247.csv',\n",
       " 'West_Coast_Bajau_west2560.csv',\n",
       " 'Southern_Paiute_sout2969.csv',\n",
       " 'Sungwaloge_(Nalemba_Simeone_Tari)_mari1426f.csv',\n",
       " 'Hinuq_hinu1240.csv',\n",
       " 'Gupapuyngu_gupa1247.csv',\n",
       " 'Archi_arch1244.csv',\n",
       " 'Ngarinyman_ngar1235.csv',\n",
       " 'Middle_High_German_midd1343.csv',\n",
       " 'Ifugaw_(Ifugao)_ifug1247.csv',\n",
       " 'Southern_Ute_utee1244.csv',\n",
       " 'Standard_Spanish_stan1288.csv',\n",
       " 'Tausug_(Sulu_Moro)_taus1251.csv',\n",
       " 'Abui_abui1241.csv',\n",
       " 'Dogrib_dogr1252.csv',\n",
       " 'Karuwali_karr1236.csv',\n",
       " 'Dupaningan_Agat_dupa1235.csv',\n",
       " 'Tasiko_tasi1237.csv',\n",
       " 'Warnman_wanm1242.csv',\n",
       " 'Warumungu_waru1265.csv',\n",
       " 'Ket_kett1243.csv',\n",
       " 'Thayore_(Kuuk_Thaayorre)_thay1249.csv',\n",
       " 'Mbyá_Guaraní_mbya1239.csv',\n",
       " 'Mussau-Emira_muss1246.csv',\n",
       " 'Bahonsuai_baho1237.csv',\n",
       " 'Puinave_puin1248.csv',\n",
       " 'Carolinians_caro1242.csv',\n",
       " 'Hehe_hehe1240.csv',\n",
       " 'Southeastern_Tepehuano_sout2976.csv',\n",
       " 'Netsilik_nets1241.csv',\n",
       " 'Wulguru_wulg1239.csv',\n",
       " 'Pampanga_(Kampampangan)_pamp1243.csv',\n",
       " 'Southern_Coastal_Tsimshian_nucl1649.csv',\n",
       " 'Sungagage_mari1426i.csv',\n",
       " 'Macedonian_mace1250.csv',\n",
       " 'Suku_suku1259.csv',\n",
       " 'Foi_foii1241.csv',\n",
       " \"Ga'dang_gada1258.csv\",\n",
       " 'Rade_(Rhade)_rade1240.csv',\n",
       " 'Pauserna_paus1244.csv',\n",
       " 'Livonian_livv1244.csv',\n",
       " 'Maori_maor1246.csv',\n",
       " 'Mekeo_meke1243.csv',\n",
       " 'Tialo_(Tomini)_tomi1243.csv',\n",
       " 'Aghul_aghu1253.csv',\n",
       " 'Amis_amis1246.csv',\n",
       " 'Malayalam_mala1464.csv',\n",
       " 'Turung_turu1249.csv',\n",
       " 'Shasta_shas1239.csv',\n",
       " 'Dargwa_Chirag__chir1284.csv',\n",
       " 'Usurufa_usar1243.csv',\n",
       " 'Akkadian_akka1240.csv',\n",
       " 'WikMungkan_wikm1247.csv',\n",
       " 'Satawalese_sata1237.csv',\n",
       " 'Kwaio_kwai1243.csv',\n",
       " 'Suki_suki1245.csv',\n",
       " 'Lala_Bisa_lala1264.csv',\n",
       " 'Senbarei_unua1237.csv',\n",
       " 'Mekeo_(West)_meke1243b.csv',\n",
       " 'Tonga_Nyasa__tong1321.csv',\n",
       " 'Inanwatan_(Suabo)_inan1242.csv',\n",
       " 'Takuu_taku1257.csv',\n",
       " 'Kamba_kamb1297.csv',\n",
       " 'Lardil_lard1243.csv',\n",
       " 'Cayubaba_cayu1262.csv',\n",
       " 'Huastec_Nahuatl_huas1257.csv',\n",
       " 'Dene_Suliné_chip1261.csv',\n",
       " 'Tombulu_tomb1243.csv',\n",
       " 'Yansi_yans1239.csv',\n",
       " 'ASF_Auslan_aust1271.csv',\n",
       " 'Umbu-Ungu_umbu1258.csv',\n",
       " 'Rotumans_rotu1241.csv',\n",
       " 'Old_Aramaic_olda1245.csv',\n",
       " 'Kambera_kamb1299.csv',\n",
       " 'Warta_Thuntai_gunt1241.csv',\n",
       " 'Meyah_meya1236.csv',\n",
       " 'Bunun_bunu1267.csv',\n",
       " 'Yarluyandi_ngam1265.csv',\n",
       " 'Maklew_makl1246.csv',\n",
       " 'Gane_(Gimán)_gane1237.csv',\n",
       " 'Itneg_(Binongan)_bino1237.csv',\n",
       " 'Arabana_arab1267a.csv',\n",
       " 'Central_Cagayan_Agta_cent2084.csv',\n",
       " 'Sardinian_sard1257.csv',\n",
       " 'Meriam_meri1244.csv',\n",
       " 'Minangkabau_mina1268.csv',\n",
       " 'Mbuun_mpuo1241.csv',\n",
       " 'Gondi_gond1265.csv',\n",
       " 'Kodi_kodi1247.csv',\n",
       " 'Mekongga_meko1237.csv',\n",
       " 'Sungwaloge_(Tawet)_mari1426c.csv',\n",
       " 'Délįne_nort2942.csv',\n",
       " 'Larevat_lare1249.csv',\n",
       " 'Rejang_(Rejangese)_reja1240.csv',\n",
       " 'Sanapana_sana1298.csv',\n",
       " 'Xalangi_(Maevo_Vanuatu)_mari1426h.csv',\n",
       " 'Ratagnon_rata1245.csv',\n",
       " 'Frisian_fris1239.csv',\n",
       " 'San_Salvador_Kongo_sans1272.csv',\n",
       " 'Warrnambool_warr1257.csv',\n",
       " 'Galeia_A_gale1259.csv',\n",
       " 'Hausa_haus1257.csv',\n",
       " 'English_stan1293.csv',\n",
       " 'Soga_soga1242.csv',\n",
       " 'Tuyuca_tuyu1244.csv',\n",
       " 'Tikopia_tiko1237.csv',\n",
       " 'Galibi_Carib_gali1262.csv',\n",
       " 'Serbs_serb1264.csv',\n",
       " 'Moni_moni1261.csv',\n",
       " 'Hawaiians_hawa1245.csv',\n",
       " 'Avá_Canoeiro_avac1239.csv',\n",
       " 'Doutai_dout1240.csv',\n",
       " 'Maba_(Indonesia)_maba1278.csv',\n",
       " 'Cubeo_cube1242.csv',\n",
       " 'Dzonghka_Wang_wang1287.csv',\n",
       " 'Kiput_kipu1237.csv',\n",
       " 'Kiowa_Apache_kiow1264.csv',\n",
       " 'Pima_de_Yepáchic_chih1238.csv',\n",
       " 'Western_Pantar_lamm1241.csv',\n",
       " 'Chenchu_chen1255.csv',\n",
       " 'Nasioi_naas1242.csv',\n",
       " 'Bera_bera1259.csv',\n",
       " 'Adnyamathanha_adny1235.csv',\n",
       " 'Kaurna_kaur1267.csv',\n",
       " 'Pohnpeian_pohn1238a.csv',\n",
       " 'Atoni_(Uab_Meto)_uabm1237.csv',\n",
       " 'Pohnpeian_pohn1238.csv',\n",
       " 'Cocama_Cocamilla_coca1259.csv',\n",
       " 'Madurese_nucl1460.csv',\n",
       " 'Baure_baur1253.csv',\n",
       " 'Lamogai_lamo1244.csv',\n",
       " 'Chepang_chep1245.csv',\n",
       " 'Bacanese_Malay_(Bacan)_baca1243.csv',\n",
       " 'Palauan_pala1344.csv',\n",
       " 'Eastern_Pahari_Nepali__east1436.csv',\n",
       " 'Bats_Tsova_Tush__bats1242.csv',\n",
       " 'Sakata_saka1287.csv',\n",
       " 'Malua_Bay_malu1245.csv',\n",
       " 'Dobu_dobu1241.csv',\n",
       " 'Luangiua_(Ontong_Java)_onto1237.csv',\n",
       " 'Nehan_neha1247.csv',\n",
       " 'Lelepa_lele1267.csv',\n",
       " 'Nalik_(Madina)_nali1244b.csv',\n",
       " 'Banjar_banj1239.csv',\n",
       " 'East_Kewa_east2516.csv',\n",
       " 'Makonde_mako1251.csv',\n",
       " 'Tidore_(version_1)_tido1248.csv',\n",
       " 'Awa_imbo1238.csv',\n",
       " 'Paluai_balu1257.csv',\n",
       " 'Yugambal_(Yugambeh)_yuga1244.csv',\n",
       " 'Kuvi_kuvi1243.csv',\n",
       " 'Malgana_malg1242.csv',\n",
       " 'Mayo_mayo1264.csv',\n",
       " 'Setaman_Baktamin_seta1246.csv',\n",
       " 'Kiwaian_sout2949.csv',\n",
       " 'Atta_atta1244.csv',\n",
       " 'Mele-Fila_(Ifira-Mele)_mele1250.csv',\n",
       " 'Sion_sion1247.csv',\n",
       " 'Ignaciano_igna1246.csv',\n",
       " 'Oroko_orok1266.csv',\n",
       " 'Tokelau_toke1240a.csv',\n",
       " 'Sake_sake1247.csv',\n",
       " 'Sika_sika1262.csv',\n",
       " 'Albanian_Tosk__tosk1239.csv',\n",
       " 'Ibanag_iban1267.csv',\n",
       " 'Waffa_waff1241.csv',\n",
       " 'Yavitero_yavi1244.csv',\n",
       " 'Samoan_samo1305a.csv',\n",
       " 'Taumako_taum1237.csv',\n",
       " 'Pagu_pagu1249.csv',\n",
       " 'Huli_huli1244.csv',\n",
       " 'Raroians_tuam1242.csv',\n",
       " 'Hiw_hiww1237.csv',\n",
       " 'Kanum_kanu1280.csv',\n",
       " 'Phende_phen1239.csv',\n",
       " 'Yulparija_yulp1239.csv',\n",
       " 'Muruwari_muru1266.csv',\n",
       " 'Atchin_atch1238.csv',\n",
       " 'Sanskrit_sans1269.csv',\n",
       " 'Duungidjawu_duun1241.csv',\n",
       " 'Bamun_bamu1253.csv',\n",
       " 'Wogeo_woge1237.csv',\n",
       " 'Badimaya_badi1246.csv',\n",
       " 'Standard_Catalan_stan1289.csv',\n",
       " 'Muna_muna1247.csv',\n",
       " 'Rarotongan_(Cook_Islands_Maori)_raro1241.csv',\n",
       " 'Nsong_song1299.csv',\n",
       " 'Avar_avar1256.csv',\n",
       " 'Dzonghka_Sha_dzon1239.csv',\n",
       " 'Niue_niue1239.csv',\n",
       " 'Ngaanyatjarra_ngaa1240.csv',\n",
       " 'Fore_fore1270.csv',\n",
       " 'Payungu_bayu1240.csv',\n",
       " 'Trumai_trum1247.csv',\n",
       " 'Kunza_kunz1244.csv',\n",
       " 'Nakanai_naka1262.csv',\n",
       " 'Owa_(Santa_Ana)_owaa1237.csv',\n",
       " 'Ndumu_ndum1239.csv',\n",
       " 'Tajik_taji1245.csv',\n",
       " 'Ngaatjatjarra_ngaa1240.csv',\n",
       " 'Nhanta_nhan1238.csv',\n",
       " 'Riantana_rian1263.csv',\n",
       " 'South_Marquesan_sout2866.csv',\n",
       " 'Ketengban_kete1254.csv',\n",
       " 'Nepali_nepa1254.csv',\n",
       " 'Kalamang_kara1499.csv',\n",
       " 'Aniwa_aniw1237.csv',\n",
       " 'Bannoni_bann1247.csv',\n",
       " 'Nage_nage1237.csv',\n",
       " 'Jaru_jaru1254.csv',\n",
       " 'Tübatülabal_tuba1278.csv',\n",
       " 'Avaz_Khunzakh__kunz1243.csv',\n",
       " 'Awetí_awet1244.csv',\n",
       " 'Moronene_moro1287.csv',\n",
       " 'Bengali_beng1280.csv',\n",
       " 'Yaqui_yaqu1251.csv',\n",
       " 'Cha_palaa_chac1249.csv',\n",
       " 'Bit_bitt1240.csv',\n",
       " 'Swedish_swed1254.csv',\n",
       " 'Proto_Sogeram_soge1235.csv',\n",
       " 'Uripiv-Wala-Rano-Atchin_urip1239.csv',\n",
       " 'Udmurt_udmu1245.csv',\n",
       " 'Kapingamarangi_kapi1249a.csv',\n",
       " 'Dargwa_Itsari__itsa1239.csv',\n",
       " 'Central_Sama_cent2092.csv',\n",
       " 'Bardi_bard1254.csv',\n",
       " 'Yerukula_yeru1240.csv',\n",
       " 'Urdu_urdu1245.csv',\n",
       " 'Waimiri_Atroari_waim1253.csv',\n",
       " 'East_Kave_east2516.csv',\n",
       " 'Midob_mido1240.csv',\n",
       " 'Chamalal_Gigatli__giga1238.csv',\n",
       " 'Daakie_port1286.csv',\n",
       " 'Batak_Toba_bata1289.csv',\n",
       " 'Sula_Mangoli_mang1408.csv',\n",
       " 'Waorani_waor1240.csv',\n",
       " 'Bali_Congo__bali1274.csv',\n",
       " 'Sungwadia_mari1426a.csv',\n",
       " 'Avar_Batlux__batl1238.csv',\n",
       " 'Toba_toba1269.csv',\n",
       " 'Toqabaqita_toab1237.csv',\n",
       " 'Lamba_lamb1271.csv',\n",
       " 'Bahvalal_bagv1239.csv',\n",
       " 'Savosavo_savo1255.csv',\n",
       " 'Standard_German_stan1295.csv',\n",
       " 'Tümpisha_Shoshoni_pana1305.csv',\n",
       " 'Irish_iris1253.csv',\n",
       " 'Umpila_umpi1239.csv',\n",
       " 'Shona_shon1251.csv',\n",
       " 'South_Tuvaluan_(Vaitupu)_sout2865.csv',\n",
       " 'Mbere_mber1257.csv',\n",
       " 'Wappo_wapp1239.csv',\n",
       " 'Mundurukú_mund1330.csv',\n",
       " 'Matukar_matu1261.csv',\n",
       " 'Ossetian_osse1243.csv',\n",
       " 'Latin_lati1261a.csv',\n",
       " 'Kuku-Yalanji_kuku1273.csv',\n",
       " 'Jahai_(Jehai)_jeha1242.csv',\n",
       " 'KomiZyrian_komi1268.csv',\n",
       " 'Ibaloi_(Nabaloi)_ibal1244.csv',\n",
       " 'Wakaya_waga1260.csv',\n",
       " 'Akhvakh_Northern_Akhvakh__akhv1239.csv',\n",
       " 'Ende_(Papua_New_Guinea)_ende1235.csv',\n",
       " 'Sumbwa_sumb1240.csv',\n",
       " 'Yandruwandha_yand1253.csv',\n",
       " 'Kom_komc1235.csv',\n",
       " 'Panjabi_panj1256.csv',\n",
       " 'Blagar_blag1240.csv',\n",
       " 'Niueans_niue1239.csv',\n",
       " 'O_du_oduu1239.csv',\n",
       " 'Kikuyu_kiku1240.csv',\n",
       " 'Pitjantjatjara_pitj1243.csv',\n",
       " 'Ancient_Greek_anci1242.csv',\n",
       " 'Western_Armenian_homs1234.csv',\n",
       " 'Nyangumarta_nyan1301.csv',\n",
       " 'Digo_digo1243.csv',\n",
       " 'Diyari_dier1241.csv',\n",
       " 'Wari__wari1268.csv',\n",
       " 'Avar_Andalal__anda1281.csv',\n",
       " 'Southeast_Ambrym_sout2859.csv',\n",
       " 'Pokomo_poko1261.csv',\n",
       " 'Yakan_yaka1277.csv',\n",
       " 'Namakura_nama1268.csv',\n",
       " 'Kariyarra_kari1304.csv',\n",
       " 'Latvian_latv1249.csv',\n",
       " 'Bilua_bilu1245.csv',\n",
       " 'Baruya_baru1267.csv',\n",
       " 'Muduapa_(Vitu)_mudu1242.csv',\n",
       " 'Rotokas_roto1249.csv',\n",
       " 'Mali_mali1284.csv',\n",
       " 'Paici_paic1239.csv',\n",
       " 'Anuta_anut1237.csv',\n",
       " 'Cheke_Holo_chek1238.csv',\n",
       " 'Maca_maca1260.csv',\n",
       " 'Czech_czec1258.csv',\n",
       " 'Malapandaram_mala1468.csv',\n",
       " 'Dyirbal_dyir1250.csv',\n",
       " 'Balochi_balo1260.csv',\n",
       " 'Dzonghka_Lhop_dzon1239.csv',\n",
       " 'Inxokvari_inxo1238.csv',\n",
       " 'Kalinga_(Ahin-Kayapa_Kalanguya)_ahin1234.csv',\n",
       " 'Mongo_mong1338.csv',\n",
       " 'Xibe_xibe1242.csv',\n",
       " 'Giryama_giry1241.csv',\n",
       " 'Nen_nenn1238.csv',\n",
       " 'Hopi_hopi1249.csv',\n",
       " 'Lozi_lozi1239.csv',\n",
       " 'Bushoong_bush1247.csv',\n",
       " 'Gunya_guny1241.csv',\n",
       " 'Waropen_waro1242.csv',\n",
       " 'Sula_Fagudu_sula1245.csv',\n",
       " 'Tzeltal_tzel1254.csv',\n",
       " 'Ayiwo_ayiw1239.csv',\n",
       " 'Yele_(Yélî_Dnye)_yele1255.csv',\n",
       " 'Kung_juho1239.csv',\n",
       " 'Pitta_Pitta_pitt1247.csv',\n",
       " 'Murrinhpatha_murr1258.csv',\n",
       " 'KukuYalanji_kuku1273.csv',\n",
       " 'Dehong_deho1238.csv',\n",
       " 'Puluwatese_(Puluwat)_pulu1242.csv',\n",
       " 'Mekeo_(North-West)_meke1243a.csv',\n",
       " 'Luvale_luva1239.csv',\n",
       " 'Marathi_mara1378.csv',\n",
       " 'Sunwadia_mari1426.csv',\n",
       " 'Kwasio_kwas1243.csv',\n",
       " 'Marwari_India__marw1260.csv',\n",
       " 'Yawa_nucl1454.csv',\n",
       " 'Wangkajunga_wang1288.csv',\n",
       " 'Gui_(Gwi)_gwii1239.csv',\n",
       " 'Central_Aymara_cent2142.csv',\n",
       " 'Eastern_Armenian_east2283.csv',\n",
       " 'Lewo_lewo1242.csv',\n",
       " 'Sawi_sawi1257.csv',\n",
       " 'Big_Nambas_bign1238a.csv',\n",
       " 'Torete_tore1237.csv',\n",
       " 'Gününa_Küne_puel1244.csv',\n",
       " 'NorthSaami_nort2671.csv',\n",
       " 'Djambarrpuyngu_djam1256.csv',\n",
       " 'Teiwa_teiw1235.csv',\n",
       " 'Waru_waru1266.csv',\n",
       " 'Nafsan_sout2856.csv',\n",
       " 'Nguna_ngun1274.csv',\n",
       " 'Easter_Islanders_rapa1244.csv',\n",
       " 'Chamalal_cham1309.csv',\n",
       " 'Waia_waia1242.csv',\n",
       " 'Paiwan_paiw1248.csv',\n",
       " 'Miningir_(Lunga)_mini1251.csv',\n",
       " 'Chavacano_chav1241.csv',\n",
       " 'Komba_komb1273.csv',\n",
       " 'Javanese_java1254.csv',\n",
       " 'Yaka_CAR__yaka1272.csv',\n",
       " 'Kui_kuii1252.csv',\n",
       " 'Yanomámi_yano1262.csv',\n",
       " 'Sangu_Tanzania__sang1330.csv',\n",
       " 'Old_Norse_oldn1244.csv',\n",
       " 'Lake_Miwok_lake1258.csv',\n",
       " 'Roviana_rovi1238.csv',\n",
       " 'Yandruwandha_yand1253a.csv',\n",
       " 'Wolio_woli1241.csv',\n",
       " 'Landewe_land1259.csv',\n",
       " 'Veps_veps1250.csv',\n",
       " 'Lingala_ling1263.csv',\n",
       " 'Sundanese_sund1252.csv',\n",
       " 'Gela_gela1263.csv',\n",
       " 'Tuvalu_(Nanumea)_tuva1244.csv',\n",
       " 'Sango_sang1331.csv',\n",
       " 'Vili_vili1238.csv',\n",
       " 'Araweté_araw1273.csv',\n",
       " 'Yombe_yomb1244.csv',\n",
       " 'Motu_motu1246.csv',\n",
       " 'Wawonii_wawo1239.csv',\n",
       " \"Ida'an_idaa1241.csv\",\n",
       " 'Proto-Oceanic_ocea1241.csv',\n",
       " \"Niuafo'ou_niua1240.csv\",\n",
       " 'Whitesands_whit1269.csv',\n",
       " 'Shoshoni_shos1248.csv',\n",
       " 'Icelandic_icel1247.csv',\n",
       " 'Puyuma_puyu1239.csv',\n",
       " 'Tupí_tupi1274.csv',\n",
       " 'Komo_DRC__komo1260.csv',\n",
       " 'Nivacle_niva1238.csv',\n",
       " 'Western_Farsi_west2369.csv',\n",
       " 'Biri_biri1256.csv',\n",
       " 'Sougb_mani1235.csv',\n",
       " 'Northern_Paiute_nort2954.csv',\n",
       " 'Emerillon_emer1243.csv',\n",
       " 'Kayardild_kaya1318.csv',\n",
       " 'Tohono_O_odham_toho1245.csv',\n",
       " 'Kwoma_kwom1262.csv',\n",
       " 'Rakahanga-Manihiki_raka1237.csv',\n",
       " 'Hewa_hewa1241.csv',\n",
       " 'Limilngan_limi1242.csv',\n",
       " 'Wallisian_(East_Uvea)_wall1257.csv',\n",
       " 'Tokharian_B_tokh1243.csv',\n",
       " 'Bila_bila1255.csv',\n",
       " 'Gayo_gayo1244.csv',\n",
       " 'Marind_nucl1622.csv',\n",
       " 'Yao_yaoo1241.csv',\n",
       " 'Standard_English_stan1293.csv',\n",
       " 'Tikopia_tiko1237a.csv',\n",
       " 'Wangkumara_wong1246.csv',\n",
       " 'Miani_mian1254.csv',\n",
       " 'Tamambo_malo1243.csv',\n",
       " 'Tubar_tuba1279.csv',\n",
       " 'Assamese_assa1263.csv',\n",
       " 'Achinese_achi1257.csv',\n",
       " 'Ngulu_ngul1246.csv',\n",
       " 'Kombai_komb1274.csv',\n",
       " 'Prai_phai1238.csv',\n",
       " 'Portuguese_port1283.csv',\n",
       " 'Wiradjuri_wira1262.csv',\n",
       " 'Bindal_bind1236.csv',\n",
       " 'Kota_kota1274.csv',\n",
       " 'South_Efate_sout2856a.csv',\n",
       " 'Kriol_Roper_River_krio1252.csv',\n",
       " \"Neve'ei_vinm1237a.csv\",\n",
       " 'Maithili_mait1250.csv',\n",
       " 'Guugu_Yimidhirr_gugu1255.csv',\n",
       " 'Kala_Lagaw_Ya_kala1377.csv',\n",
       " 'North_Efate_(Nguna,_Nakanamanga)_nort2836.csv',\n",
       " 'Parakanã_para1312.csv',\n",
       " 'Watam_wata1253.csv',\n",
       " 'Mota_mota1237.csv',\n",
       " 'Old_Javanese_(Kawi)_kawi1241.csv',\n",
       " 'Rahambuu_raha1237.csv',\n",
       " 'Tadyawan_tady1237.csv',\n",
       " 'Sangtam_Naga_sang1321.csv',\n",
       " 'Umiray_Dumaget_Agta_umir1236.csv',\n",
       " 'Yindjibarndi_yind1247.csv',\n",
       " 'Rutul_shin1265.csv',\n",
       " 'Tanala_tana1285.csv',\n",
       " 'Guarayu_guar1292.csv',\n",
       " 'Adang_adan1251.csv',\n",
       " 'Margany_marg1253.csv',\n",
       " 'Yugambeh_yugu1249.csv',\n",
       " 'Nevome_pima1248b.csv',\n",
       " 'Hittite_hitt1242.csv',\n",
       " 'Taiap_taia1239.csv',\n",
       " 'Yalarnnga_yala1262a.csv',\n",
       " 'Bininj_Kun-wok_gunw1252.csv',\n",
       " 'Duma_duma1253.csv',\n",
       " 'Kaonde_kaon1241.csv',\n",
       " 'Nanggu_nang1262.csv',\n",
       " 'Kalam_kala1397.csv',\n",
       " 'Vao_vaoo1237.csv',\n",
       " 'Konda_(Western_Dani)_west2594.csv',\n",
       " 'Luxembourgish_luxe1241.csv',\n",
       " 'Kadai_kada1286.csv',\n",
       " 'Nese_nese1235.csv',\n",
       " 'German_stan1295.csv',\n",
       " 'Telugu_telu1262.csv',\n",
       " 'Eyak_eyak1241.csv',\n",
       " 'Rotinese_Termanu__term1237.csv',\n",
       " 'Hunzib_hunz1247.csv',\n",
       " 'Suzhou_suzh1234.csv',\n",
       " 'Ha_haaa1252.csv',\n",
       " 'Songoora_song1300.csv',\n",
       " 'Djinang_djin1253.csv',\n",
       " 'Karitiana_kari1311.csv',\n",
       " 'Aria_(Mouk-Aria)_mouk1240a.csv',\n",
       " 'Northwestern_Maidu_nort2951.csv',\n",
       " 'Yuki_yuki1243.csv',\n",
       " 'Routa_rout1237.csv',\n",
       " 'Kamayurá_kama1373.csv',\n",
       " 'Kulisusu_kuli1254.csv',\n",
       " 'Vures_vure1239.csv',\n",
       " 'Amanab_aman1265.csv',\n",
       " 'Bali_DRC__bali1275.csv',\n",
       " 'Klon_kelo1247.csv',\n",
       " 'Lubuagan_Kalinga_lubu1243.csv',\n",
       " 'Touo_touo1238.csv',\n",
       " 'Prakaa_prak1243.csv',\n",
       " 'Nama_nama1266.csv',\n",
       " 'Itbayat_ivat1242.csv',\n",
       " 'Western_Tawbuid_(Batangan)_west2559.csv',\n",
       " 'Bihari_biha1245.csv',\n",
       " 'Sangu_sang1333.csv',\n",
       " 'Kartujarra_kart1247.csv',\n",
       " 'Eastern_Yiddish_east2295.csv',\n",
       " 'Kilivila_kili1267.csv',\n",
       " 'Judeo_Tat_jude1256.csv',\n",
       " 'North_Northern_Paiute_nort1551.csv',\n",
       " 'Kaiwá_kaiw1246.csv',\n",
       " 'Tokelau_toke1240.csv',\n",
       " 'Ekari_ekar1243.csv',\n",
       " 'Thao_thao1240.csv',\n",
       " 'TundraNenets_nene1249.csv',\n",
       " 'Luba_Katanga_luba1250.csv',\n",
       " 'Pukapuka_puka1242.csv',\n",
       " 'Bafia_bafi1243.csv',\n",
       " 'Dhuwal_dhuw1249.csv',\n",
       " 'Magyar_hung1274.csv',\n",
       " 'Watut_watu1246.csv',\n",
       " 'Patani_pata1260.csv',\n",
       " 'Albanian_alba1267.csv',\n",
       " 'Munggava_(Rennell)_mung1271.csv',\n",
       " 'Jurúna_juru1256.csv',\n",
       " 'Tundra_Nenets_nene1249.csv',\n",
       " 'Bari_bari1297.csv',\n",
       " 'Batanga_bata1285.csv',\n",
       " 'Diyari_dier1241a.csv',\n",
       " 'Western_Shoshoni_west2622.csv',\n",
       " 'Emau_(Emao_Island)_emau1237.csv',\n",
       " 'Idi_nucl1597.csv',\n",
       " 'Lengua_leng1262.csv',\n",
       " 'Buhid_buhi1245.csv',\n",
       " 'Barok_(Karu)_baro1253.csv',\n",
       " 'Tongan_tong1325.csv',\n",
       " 'Tehuelche_tehu1242.csv',\n",
       " 'Kadar_kada1242.csv',\n",
       " 'East_Futuna_east2447.csv',\n",
       " 'Jicarilla_jica1244.csv',\n",
       " 'Mori_Bawah_mori1268.csv',\n",
       " 'Tiri-Mea_(Grand_Couli)_tiri1258.csv',\n",
       " 'Manam_mana1295.csv',\n",
       " 'Temi_temi1247.csv',\n",
       " 'Wuvulu_Aua_wuvu1239.csv',\n",
       " 'Wayuu_wayu1243.csv',\n",
       " 'Kodeoha_kode1237.csv',\n",
       " 'Fortsenal_(Kiai)_fort1240.csv',\n",
       " 'Muisca_Chibcha__chib1270.csv',\n",
       " 'Tswana_tswa1253.csv',\n",
       " 'Old_English_ca_450_1100__olde1238.csv',\n",
       " \"Ili'uun_iliu1237.csv\",\n",
       " 'Au_auuu1241.csv',\n",
       " 'Middle_English_midd1317.csv',\n",
       " 'Khalkha-Mongolian_mong1331.csv',\n",
       " 'Hunza_hunz1248.csv',\n",
       " 'Herero_here1253.csv',\n",
       " 'Copper_Inuit_copp1244.csv',\n",
       " 'Mahasu_Pahari_maha1287.csv',\n",
       " 'Zulu_zulu1248.csv',\n",
       " 'Bukat_buka1261.csv',\n",
       " 'Gurindji_guri1247.csv',\n",
       " 'Elamite_elam1244.csv',\n",
       " 'Kristang_(Malacca_Creole_Portugese)_mala1533.csv',\n",
       " 'Awiakay_awia1234.csv',\n",
       " 'Saaroa_saar1237.csv',\n",
       " \"I'saka_(Krisa)_kris1246.csv\",\n",
       " 'Mortlockese_mort1237.csv',\n",
       " 'Yalarnnga_yala1262.csv',\n",
       " 'Nahavaq_sout2857.csv',\n",
       " 'Hanunoo_hanu1241.csv',\n",
       " 'Alyawarr_alya1239.csv',\n",
       " \"Kuuku_Ya'u_kuuk1238.csv\",\n",
       " 'Nogai_noga1249.csv',\n",
       " 'Tsafiki_colo1249.csv',\n",
       " 'Hopi_S__hopi1249b.csv',\n",
       " 'Sungwadaga_mari1426b.csv',\n",
       " 'Ossetic_osse1245.csv',\n",
       " 'Wersing_wers1238.csv',\n",
       " 'Wiyot_wiyo1248.csv',\n",
       " 'Warungu_waru1264.csv',\n",
       " 'PintupiLuritja_pint1250.csv',\n",
       " 'Maskelynes_mask1242.csv',\n",
       " 'Mangaia_mang1402.csv',\n",
       " 'Ingush_ingu1240.csv',\n",
       " 'Tupinambá_tupi1273.csv',\n",
       " 'Mudburra_mudb1240.csv',\n",
       " 'Wapishana_wapi1253.csv',\n",
       " 'Suundi_suun1239.csv',\n",
       " 'Urak_Lawoi_urak1238.csv',\n",
       " 'Selkup_selk1253.csv',\n",
       " 'Yamdena_yamd1240.csv',\n",
       " 'Djinang_djin1253a.csv',\n",
       " 'Jamaican_Creole_jama1262.csv',\n",
       " 'Kuanua_kuan1248.csv',\n",
       " 'Tetela_tete1250.csv',\n",
       " 'Saterfriesisch_sate1242.csv',\n",
       " 'Sonsorol_sons1242.csv',\n",
       " 'Itonama_iton1250.csv',\n",
       " 'Talibabu_tali1262.csv',\n",
       " 'Mwotlap_motl1237.csv',\n",
       " 'Kélé_kele1257.csv',\n",
       " 'Movima_movi1243.csv',\n",
       " 'Yale_kosa1249.csv',\n",
       " 'Cornish_corn1251.csv',\n",
       " 'Pashto_pash1269.csv',\n",
       " 'Fijian_fiji1243.csv',\n",
       " 'Yuracare_yura1255.csv',\n",
       " 'MeadowMari_gras1239.csv',\n",
       " 'Kilmeri_kilm1241.csv',\n",
       " 'Budukh_budu1248.csv',\n",
       " 'Tulambatu_tula1253.csv',\n",
       " 'Western_Bukidnon_Manobo_west2555.csv',\n",
       " 'Italian_ital1282.csv',\n",
       " 'Cupeño_cupe1243.csv',\n",
       " 'Avava_(Niviar_Dialect)_katb1237b.csv',\n",
       " 'Buli_buli1256.csv',\n",
       " 'De_kwana_Maquiritari__maqu1238.csv',\n",
       " 'Oriya_oriy1255.csv',\n",
       " 'Yidiny_yidi1250.csv',\n",
       " 'Uradhi_(Atampaya)_atam1239.csv',\n",
       " 'Bario_Kelabit_bari1288.csv',\n",
       " 'Tajio_taji1246.csv',\n",
       " 'Ritharrngu_rita1239.csv',\n",
       " 'Meta_meta1238.csv',\n",
       " 'Kamang_kama1365.csv',\n",
       " 'Iau_iauu1242.csv',\n",
       " 'Koiari_gras1249.csv',\n",
       " 'Páez_paez1247.csv',\n",
       " 'Basque_west1508.csv',\n",
       " 'Wayampi_waya1270.csv',\n",
       " 'Ngiyambaa_wang1291.csv',\n",
       " 'Mocoví_moco1246.csv',\n",
       " 'Lalomerui_lalo1239.csv',\n",
       " 'Anambé_anam1249.csv',\n",
       " 'Erzya_erzy1239.csv',\n",
       " 'Sindhi_sind1272.csv',\n",
       " 'Chuukese_(Trukese)_chuu1238.csv',\n",
       " 'Gumbaynggir_kumb1268.csv',\n",
       " 'Kiwaian_kiwa1251.csv',\n",
       " 'Bende_bend1258.csv',\n",
       " 'Kumyk_kumy1244.csv',\n",
       " 'Iban_iban1264.csv',\n",
       " 'HillMari_west2392.csv',\n",
       " 'Tausug_taus1251a.csv',\n",
       " 'Breton_bret1244.csv',\n",
       " 'Chimariko_chim1301.csv',\n",
       " 'Anejom_anei1239.csv',\n",
       " 'Finnic_finn1318.csv',\n",
       " 'Old_Irish_oldi1245.csv',\n",
       " 'Lau_Fijian_laua1243.csv',\n",
       " 'Monumbo_nucl1458.csv',\n",
       " 'Kiraman_kira1248.csv',\n",
       " 'Ganggalida_gang1267.csv',\n",
       " 'Pumé_pume1238.csv',\n",
       " 'Cusco_Quechua_cusc1236.csv',\n",
       " 'Daribi_dadi1250.csv',\n",
       " 'Mafea_(Mavea)_mafe1237.csv',\n",
       " 'Tolowa_tolo1259.csv',\n",
       " 'Lauje_lauj1238.csv',\n",
       " 'Avava_katb1237a.csv',\n",
       " 'Wailaki_wail1244.csv',\n",
       " 'Araki_arak1252.csv',\n",
       " 'Kyaka_kyak1244.csv',\n",
       " 'Rangiora_(NW_Tuamotus)_tuam1242.csv',\n",
       " 'Mapudungun_mapu1245.csv',\n",
       " 'Mono_mono1275.csv',\n",
       " 'Mari_east2328.csv',\n",
       " 'Suzhou_shan1293.csv',\n",
       " 'Tucanoan_Bará_waim1255.csv',\n",
       " 'Tomadino_toma1248.csv',\n",
       " 'Umbundu_umbu1257.csv',\n",
       " 'Seri_seri1257.csv',\n",
       " 'Cruzeño_cruz1243.csv',\n",
       " 'Gusii_gusi1247.csv',\n",
       " 'Asera_aser1237.csv',\n",
       " 'Old_Rapa_(Rapa)_rapa1245.csv',\n",
       " 'Yucatec_Maya_yuca1254.csv',\n",
       " 'Erave_(South_Kewa)_erav1244.csv',\n",
       " 'Neme_neme1244.csv',\n",
       " 'SouthSaami_sout2674.csv',\n",
       " 'Tongarevans_penr1237.csv',\n",
       " 'Igbo_nucl1417.csv',\n",
       " 'Raga_(Hano_Thomas_Ennever)_hano1246b.csv',\n",
       " 'Hiligaynon_hili1240.csv',\n",
       " 'DeneTha_South_Slavey__sout2959.csv',\n",
       " 'Emae_(Mae)_emae1237.csv',\n",
       " 'Tangoa_(Movono)_tang1347.csv',\n",
       " 'Qawasquar_qawa1238.csv',\n",
       " 'Tabla_tabl1243.csv',\n",
       " 'Raga_(Hano)_hano1246.csv',\n",
       " 'Mangei_(Sobjo)_mang1407.csv',\n",
       " 'North_Tanna_nort2847.csv',\n",
       " 'Kazym_Berezover_Suryskarer_Khanty_khan1273.csv',\n",
       " 'Wajarri_waja1257.csv',\n",
       " 'Kogi_cogu1240.csv',\n",
       " 'Yidiny_yidi1250a.csv',\n",
       " 'Kerewe_kere1283.csv',\n",
       " 'Central_Kurdish_cent1972.csv',\n",
       " 'Tokharian_A_tokh1242.csv',\n",
       " 'Koroni_koro1311.csv',\n",
       " 'Big_Nambas_bign1238.csv',\n",
       " 'Lavatbura-lamusong_(Konobin)_lava1239.csv',\n",
       " 'East_Ambae_(Lolovoli)_east2443.csv',\n",
       " 'Warlmanpa_warl1255.csv',\n",
       " 'Iyojwa_ja_Chorote_iyoj1235.csv',\n",
       " 'Naman_litz1237.csv',\n",
       " 'Panare_enap1235.csv',\n",
       " 'Old_Welsh_oldw1239.csv',\n",
       " 'Estonian_esto1258.csv',\n",
       " 'Nungon_yaum1237.csv',\n",
       " 'Ura_urav1235.csv',\n",
       " 'West_Kewa_west2599.csv',\n",
       " 'Tsogo_tsog1243.csv',\n",
       " 'Gabrielino_tong1329.csv',\n",
       " 'Maragus_(Tape)_mara1399.csv',\n",
       " 'Weda_weda1240.csv',\n",
       " 'MartuWangka_mart1256.csv',\n",
       " 'Zabana_(Kia)_zaba1237.csv',\n",
       " 'Myene_myen1241.csv',\n",
       " 'Nalik_(Lukuramau)_nali1244.csv',\n",
       " 'Russian_russ1263.csv',\n",
       " 'Car_Nicobarese_carn1240.csv',\n",
       " 'Chipaya_chip1262.csv',\n",
       " 'Bandjalang_band1339.csv',\n",
       " 'Old_Prussian_prus1238.csv',\n",
       " 'Barabet_(Hano)_hano1246a.csv',\n",
       " 'Ding_ding1239.csv',\n",
       " 'Wallon_wall1255.csv',\n",
       " 'North_Azerbaijani_nort2697.csv',\n",
       " 'Bontok_bont1247.csv',\n",
       " 'Nganasan_ngan1291.csv',\n",
       " 'Paraguayan_Guaraní_para1311.csv',\n",
       " 'Masaaba_masa1299.csv',\n",
       " 'Yuwaalaraay_gami1243.csv',\n",
       " 'Yorta_Yorta_yort1237.csv',\n",
       " 'Manange_mana1288.csv',\n",
       " 'Boano_(Sulawesi)_boan1243.csv',\n",
       " 'Suruí_suru1262.csv',\n",
       " 'Yamana_yama1264.csv',\n",
       " 'Kumaoni_kuma1273.csv',\n",
       " 'Koya_koya1251.csv',\n",
       " 'Madak_(Konnos)_mada1284.csv',\n",
       " 'Ngarluma_ngar1287.csv',\n",
       " 'Nyamal_nyam1271.csv',\n",
       " 'Arizona_Yaqui_yaqu1251a.csv',\n",
       " 'Arabana_arab1267.csv',\n",
       " 'Ngadjumaya_ngad1258.csv',\n",
       " 'Yurok_yuro1248.csv',\n",
       " 'Karok_karo1304.csv',\n",
       " \"Riuk_Bekati'_(Dayak)_beka1241.csv\",\n",
       " \"'Are'are_area1240.csv\",\n",
       " 'Luba_Lulua_luba1249.csv',\n",
       " 'Tidore_tido1249.csv',\n",
       " 'Northern_Tepehuan_nort2959.csv',\n",
       " 'Fefe_fefe1239.csv',\n",
       " 'Sateré_Mawé_sate1243.csv',\n",
       " \"Tokotu'a_(Kabaena)_kaba1285.csv\",\n",
       " 'Budza_budz1238.csv',\n",
       " 'Eudeve_eude1234.csv',\n",
       " 'Sarangani_Blaan_sara1326.csv',\n",
       " 'Duala_dual1243.csv',\n",
       " 'Turkish_nucl1301.csv',\n",
       " 'Kunjen_kunj1248.csv',\n",
       " 'Pilagá_pila1245.csv',\n",
       " 'Biak_biak1248.csv',\n",
       " 'Mokpwe_mokp1239.csv',\n",
       " 'Tarahumara_cent2131.csv',\n",
       " 'Alamblak_alam1246.csv',\n",
       " 'Northeastern_Maidu_nort2952.csv',\n",
       " 'Cotabato_Manobo_cota1241.csv',\n",
       " 'Russian_russ1263a.csv',\n",
       " 'Guajá_guaj1256.csv',\n",
       " 'Negwa_(Yagwoia)_yagw1240.csv',\n",
       " 'Seke_(Ske)_seke1241.csv',\n",
       " 'Kukatja_kuka1246.csv',\n",
       " 'Vaekau-Taumako_pile1238.csv',\n",
       " 'Tonga_(Tonga_Islands)_tong1325.csv',\n",
       " 'Rundi_rund1242.csv',\n",
       " 'Wai_Wai_waiw1244.csv',\n",
       " 'Colorado_Ute_utee1244a.csv',\n",
       " 'Tagalog_taga1270.csv',\n",
       " 'Merei_mere1242.csv',\n",
       " 'ForestEnets_fore1265.csv',\n",
       " 'Wichí_wich1264.csv',\n",
       " 'Opata_opat1246.csv',\n",
       " 'Tukang_Besi_South_tuka1249.csv',\n",
       " 'Bezhta_bezh1248.csv',\n",
       " 'Sungwaloge_(Tauta)_mari1426e.csv',\n",
       " 'Lisu_lisu1250d.csv',\n",
       " 'Djapu_dhuw1249.csv',\n",
       " 'Lametin_mere1242.csv',\n",
       " 'Sie_(Sye)_siee1239.csv',\n",
       " 'Dutch_dutc1256.csv',\n",
       " 'Lahnda_lahn1241.csv',\n",
       " 'Huichol_huic1243.csv',\n",
       " 'Ingrian_ingr1248.csv',\n",
       " 'Eipo_eipo1242.csv',\n",
       " 'Paliyan_pali1274.csv',\n",
       " 'Sherpa_sher1255.csv',\n",
       " 'Isirawa_isir1237.csv',\n",
       " 'Langi_lang1320.csv',\n",
       " 'Hupa_hupa1239.csv',\n",
       " 'White_Mesa_Ute_utee1244b.csv',\n",
       " 'Yaka_Pelende_Lonzo_yaka1269.csv',\n",
       " 'Latin_lati1261.csv',\n",
       " 'Dampelas_damp1237.csv',\n",
       " 'NorthMuyu_nort2916.csv',\n",
       " 'Pemon_pemo1248.csv',\n",
       " 'Mono-Alu_mono1273.csv',\n",
       " 'Hoti_Yuwana__yuwa1244.csv',\n",
       " 'Waurá_waur1244.csv',\n",
       " 'Kitanemuk_serr1255b.csv',\n",
       " 'Wambaya_wamb1258.csv',\n",
       " 'Dari_dari1249.csv',\n",
       " 'Meru_meru1245.csv',\n",
       " 'Tubuai_tubu1240.csv',\n",
       " 'Cora_elna1235.csv',\n",
       " 'Northern_Haida_nort2938.csv',\n",
       " 'Ifaluk_Nuclear_Woleaian__nucl1479.csv',\n",
       " 'MokshaMordvin_moks1248.csv',\n",
       " 'Mashco_Piro_mash1270.csv',\n",
       " 'Kurukh_kuru1302.csv',\n",
       " 'Klon_klon1234.csv',\n",
       " 'Kapingamarangi_kapi1249.csv',\n",
       " 'Sikaiana_sika1261.csv',\n",
       " 'Lithuanian_lith1251.csv',\n",
       " 'Northern_Kurdish_nort2641.csv',\n",
       " 'Venda_vend1245.csv',\n",
       " 'Boers_afri1274.csv',\n",
       " 'Serrano_serr1255.csv',\n",
       " 'Epena_epen1239.csv',\n",
       " 'Ulawa_ulaw1237.csv',\n",
       " 'Wipi_wipi1242.csv',\n",
       " 'Telefol_tele1256.csv',\n",
       " 'Rennell_Islanders_renn1242.csv',\n",
       " 'Guugu_Yimidhirr_gugu1255a.csv',\n",
       " 'Slovenian_slov1268.csv',\n",
       " 'Toda_toda1252.csv',\n",
       " 'Magam_maga1262.csv',\n",
       " 'Tirax_(Mae)_maee1241.csv',\n",
       " 'Cofan_cofa1242.csv',\n",
       " 'ErzyaMordvin_erzy1239.csv',\n",
       " 'Luburua_(Kara)_lubu1242.csv',\n",
       " 'Idi_idii1243.csv',\n",
       " 'Nagovisi_sibe1248.csv',\n",
       " 'Atayal_atay1247.csv',\n",
       " \"Brooke's_Point_Palawano_broo1239.csv\",\n",
       " 'Selk_nam_onaa1245.csv',\n",
       " 'Loloda-Laba_(Loda)_lolo1264.csv',\n",
       " 'Guarijio_huar1255.csv',\n",
       " 'Ngarla_ngar1286.csv',\n",
       " 'Jingulu_djin1251.csv',\n",
       " 'Southern_Tepehuan_sout2975.csv',\n",
       " 'Shi_shii1238.csv',\n",
       " 'Tungag_tung1290.csv',\n",
       " 'Comanche_coma1245.csv',\n",
       " 'Cebuano_cebu1242.csv',\n",
       " 'Mala_mala1294.csv',\n",
       " 'Alangan_alan1249.csv',\n",
       " 'Tamil_tami1289.csv',\n",
       " 'Kwamera_kwam1252.csv',\n",
       " 'Konawe_kona1243.csv',\n",
       " 'Port_Sandwich_port1285.csv',\n",
       " 'Gothic_goth1244.csv',\n",
       " \"Neve'ei_vinm1237.csv\",\n",
       " 'Northern_Gondi_nort2702.csv',\n",
       " 'Kalkatungu_kalk1246.csv',\n",
       " 'Aché_ache1246.csv',\n",
       " 'Mungaka_mung1266.csv',\n",
       " \"Ma'anyan_maan1238.csv\",\n",
       " 'Luiseńo_luis1253.csv',\n",
       " 'Northern_Ojibwa_nort2961.csv',\n",
       " 'Nyawaygi_nyaw1247.csv',\n",
       " 'Modern_Greek_mode1248.csv',\n",
       " 'Kashaya_kash1280.csv',\n",
       " \"Vera'a_vera1241.csv\",\n",
       " 'Karajarri_kara1476.csv',\n",
       " 'Bierebo_bier1244.csv',\n",
       " 'Mekeo_(East)_meke1243d.csv',\n",
       " 'Zuni_zuni1245.csv',\n",
       " 'Central_Subanen_(Subanung)_cent2089.csv',\n",
       " 'Mori_Atas_mori1269.csv',\n",
       " 'Shipibo_ship1254.csv',\n",
       " 'Kazakh_kaza1248.csv',\n",
       " 'Kankanay_nort2877.csv',\n",
       " 'Ngarrindjeri_narr1259.csv',\n",
       " 'Vlax_Romani_vlax1238.csv',\n",
       " 'Bulgarian_bulg1262.csv',\n",
       " 'Mangarevans_mang1401.csv',\n",
       " 'Sinhala_sinh1246.csv',\n",
       " 'Manggarai_mang1405.csv',\n",
       " 'Pirriya_pirr1240.csv',\n",
       " 'Keyagana_(Jate)_keya1238.csv',\n",
       " 'Danish_dani1285.csv',\n",
       " 'Chokwe_chok1245.csv',\n",
       " 'Hungarian_hung1274.csv',\n",
       " 'Proto_Tupi_Guarani_tupi1276.csv',\n",
       " 'Tanampedagi_Taje_taje1237b.csv',\n",
       " 'Wathwurrung_wath1238.csv',\n",
       " 'Mungiki_(Bellona)_mung1270.csv',\n",
       " 'Marshallese_mars1254.csv',\n",
       " 'Totoli_toto1304.csv',\n",
       " 'Amharic_amha1245.csv',\n",
       " 'Chiriguano_east2555.csv',\n",
       " 'Hindi_hind1269.csv',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kb_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508a7a7",
   "metadata": {},
   "source": [
    "Using one of these filenames, we can extract the kin terminology from that file and populate a dictionary with it. We're only interested in two columns from the Kinbank data: `parameter`, which contains a short code indicating a **kin type**, and `word`, which contains the **kin term** associated with that kin type. An example of a row in the English data would be `mMeB, uncle`, where `mMeB` means 'male speaker's mother's older brother', and `uncle` is the term associated with that person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d98021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kin_terms(filepath: str) -> dict:\n",
    "    ks = {}\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        next(csv_reader) # to skip the header row\n",
    "        for line in csv_reader:\n",
    "            kin_type = line['parameter']\n",
    "            kin_term = line['word']\n",
    "            kin_term = kin_term.split(',')[0]\n",
    "            if '(' in kin_term:\n",
    "                kin_term = kin_term.split('(')[0][:-1]\n",
    "            ks[kin_type] = kin_term\n",
    "    return ks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2019",
   "metadata": {},
   "source": [
    "### 2. Condense the system down\n",
    "\n",
    "We're interested in the mutual information between the kin terms in Generation 0 (Ego's generaiton) and Generation +1 (Ego's parents' generation).\n",
    "\n",
    "We want create a data structure that pairs up parent types with the corresponding child types. This is because we're interested in whether kinship systems maintain patterns of terminological distinctions and mergers across these two generations, so we will need to know which parent terms 'go with' which child terms.\n",
    "\n",
    "In `kintypes`, you will find a list of pairs of kin types, where the first element in the pair is a parent type, and the second is their child; e.g. mMeB and mMeBD (mother's elder brother and mother's elder brother's daughter). \n",
    "\n",
    "`get_pairs()` takes a kinship system as input, and outputs a list of tuples. The first element in the tuple is the parent term, the second is the corresponding child term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a64747f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(ks: dict) -> list:\n",
    "    pairs_of_terms = []\n",
    "    parent_types = []\n",
    "\n",
    "    for pair in kt.ics_pairs:\n",
    "        if pair[0] in ks and pair[1] in ks:\n",
    "            pairs_of_terms.append((ks[pair[0]],ks[pair[1]]))\n",
    "            parent_types.append(pair[0])\n",
    "                \n",
    "    return pairs_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b7174",
   "metadata": {},
   "source": [
    "But for our calculations, we'll still need to know which terms belong to which generation. Luckily, we know that the 0th element in each tuple is from Ego's parents' generation and the 1st element is from Ego's generation. So we can happily split these tuples down the middle and populate two lists with the terms. `split_pairs()` takes a list of pairs and sorts it into terms that belong to Ego's generation and terms that belong to Ego's parents' generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d001bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pairs(pairs: list) -> list:\n",
    "    gn = []\n",
    "    gn1 = []\n",
    "    for pair in pairs:\n",
    "        gn.append(pair[1])\n",
    "        gn1.append(pair[0])\n",
    "    \n",
    "    return gn,gn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc975d",
   "metadata": {},
   "source": [
    "### 3. Calculate probabilities\n",
    "\n",
    "To calculate entropy, we need a probability distribution over the terms in one single generation of a kinship system. So let's start with a function that can calculate the probability of a particular term.\n",
    "\n",
    "Given a term and the full list of terms in the same generation, `probability()` counts how many times that term exists in `generation` and divides that by the total length of `generation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a04be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(term: str, generation: list) -> float:\n",
    "    return generation.count(term)/len(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de612e",
   "metadata": {},
   "source": [
    "When calculating mutual information, we also need the **conditional entropy** between the two generations of our system. To calculate this, we will need not only the probabilities of terms in a generation, but also the **joint probabilities** of every pair of terms across those two generations. In other words, we need to calculate the probabilities of our `get_pairs` output.\n",
    "\n",
    "Given two terms, `joint_probability()` counts how many pairs made of those two terms exist in `pairs`, then divides that by the total length of `pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ec2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_probability(term1: str, term2: str, pairs: list) -> float:\n",
    "    pair = (term1,term2)\n",
    "    return pairs.count(pair)/len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed267dd",
   "metadata": {},
   "source": [
    "### 4. Calculating entropy and mutual information\n",
    "\n",
    "Entropy (in bits) is defined as \n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{x \\in X}p(x) log_2p(x)\n",
    "$$\n",
    "\n",
    "or, in English, it is the inverse sum over a distribution X of the probability of y * the log probability of y.\n",
    "\n",
    "Entropy is a measure of the average level of uncertainty about the possible outcomes of a variable.\n",
    "\n",
    "First, let's define a function `entropy()` that will iterate over a generation of the kinship system and output the entropy of that generation. \n",
    "\n",
    "Note: we only need one generation's entropy score to calculate mutual information - we make the arbitrary choice to calculate the entropy of Ego's parents' generation later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9cbfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(generation: list) -> list:\n",
    "    entropy = 0\n",
    "    for term in set(generation): # using a set as we want to count each unique term only once\n",
    "        p = probability(term,generation)\n",
    "        #print('entropy of',term,p*math.log(p))\n",
    "        entropy += p*math.log2(p)\n",
    "    return round(-entropy,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0561a",
   "metadata": {},
   "source": [
    "Conditional entropy of Y given X is defined as\n",
    "\n",
    "$$\n",
    "H(Y|X) = -\\sum_{x \\in X,y \\in Y}p(x,y) log_2 {p(x,y) \\over p(x)}\n",
    "$$\n",
    "\n",
    "or in English, the inverse sum over two distributions Y and X of the probability of each y * the log probability of each y given x.\n",
    "\n",
    "Conditional entropy is the amount of information needed to describe the outcome of a random variable Y given that we already know the value of another random variable X.\n",
    "\n",
    "To calculate it, we need the joint probability of each pair (given by `joint_probability()`) and the probability of one member of that pair (given by `probability()`). We can then calculate the conditional probability of parent term given child term as the joint probability of those terms over the probability of the parent term.\n",
    "\n",
    "`conditional_entropy()` iterates over all pairs to output the conditional entropy of Ego's generation given Ego's parents' generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661d794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(gn: list, pairs:list) -> float:\n",
    "    entropy = 0\n",
    "    for x,y in set(pairs): # x = parent, y = child\n",
    "        p_xy = joint_probability(x,y,pairs)\n",
    "        p_y = probability(y,gn)\n",
    "        if p_xy > 0 and p_y > 0:\n",
    "            #print('p(', x, '|', y,') = ', p_xy/p_y, 'p(y) = ', p_y)\n",
    "            entropy += p_xy * math.log2(p_xy/p_y)\n",
    "    return round(-entropy,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725714bb",
   "metadata": {},
   "source": [
    "Finally, mutual information is defined as\n",
    "\n",
    "$$\n",
    "I(X;Y) \\equiv H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "or in English, entropy of X minus the conditional entropy of X given Y.\n",
    "\n",
    "In this study, it is equal to the entropy of Ego's parents' generation minus the conditional entropy of Ego's parents' generation given Ego's generation. It tells us how much mutual dependence there is between these two generations; i.e. how much we can predict about one by observing the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e3a4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(pairs: list):\n",
    "    gn,gn1 = split_pairs(pairs)\n",
    "    e = entropy(gn1)\n",
    "    ce = conditional_entropy(gn,pairs)\n",
    "    mi = e - ce\n",
    "    return round(mi,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a0932b1-af8d-4365-942e-c0e37f312633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mutual_information(e,ce):\n",
    "#     return round(e - ce, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa97164",
   "metadata": {},
   "source": [
    "### 5. Calculate MI for each language and save data\n",
    "\n",
    "With all of the above infrastructure, we can calculate the mutual information between G0 and G+1 for all the languages in Kinbank and save this data to a separate .csv file.\n",
    "\n",
    "First, a function that calculates the relevant data and stores it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b67aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(pairs,results):\n",
    "    gn,gn1 = split_pairs(pairs)\n",
    "    egn = entropy(gn)\n",
    "    egn1 = entropy(gn1)\n",
    "    ce = conditional_entropy(gn,pairs)\n",
    "    mi = mutual_information(pairs)\n",
    "    \n",
    "    results['mutual_information'] = mi\n",
    "    results['entropy_gn'] = egn\n",
    "    results['entropy_gn1'] = egn1\n",
    "    results['conditional_entropy'] = ce\n",
    "    results['variation_gn'] = len(set(gn))\n",
    "    results['variation_gn1'] = len(set(gn1))\n",
    "    results['number_of_pairs'] = len(set(pairs))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb334db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "families = ['Afro-Asiatic','Algic','Arawakan','Atlantic-Congo','Austroasiatic','Austronesian','Cariban','Dravidian','Indo-European',\n",
    "            'Nakh-Daghestanian','Nuclear Trans New Guinea', 'Other','Pama-Nyungan','Pano-Tacanan','Salishan','Sino-Tibetan',\n",
    "            'Tai-Kadai','Tupian','Turkic','Uralic','Uto-Aztecan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f441311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MI(families: list,filename: str):\n",
    "    df = []\n",
    "    codes = []\n",
    "    filepath = '../../kinbank-family/'\n",
    "    \n",
    "    for family in families:\n",
    "        all_files = get_kb_files(filepath + family)\n",
    "        \n",
    "        for file in all_files:\n",
    "            print(file)\n",
    "            match = re.search(\"[a-z]{4}[0-9]{4}[a-z]?\\\\.\", file)\n",
    "            code = match.group()\n",
    "            code = code[:len(code)-1]\n",
    "            language = file.split('_' + code)[0]\n",
    "            \n",
    "            if 'Morgan' in language:\n",
    "                match = re.search(\"Morgan[0-9]{4}\",language)\n",
    "                morgan = match.group()\n",
    "                language = language.split(morgan + '_')[1]\n",
    "\n",
    "            if code not in codes:\n",
    "                codes.append(code)\n",
    "\n",
    "                ks = get_kin_terms(filepath + family + '/' + file)\n",
    "\n",
    "                pairs = get_pairs(ks)\n",
    "                \n",
    "                # g0,g1 = split_ks(ks)\n",
    "                g0,g1 = split_pairs(pairs)\n",
    "\n",
    "                g0_relatives,g1_relatives = split_ks(ks)\n",
    "\n",
    "                if pairs: # if pairs is not empty\n",
    "\n",
    "                    results = {}\n",
    "                    results['language'] = language\n",
    "                    results['language_family'] = family\n",
    "                    results['code'] = code\n",
    "                    results['simulation_code'] = code + '_REAL'\n",
    "                    results['simulation'] = 'N'\n",
    "                    # results['mutual_information'] = entropy(list(g1.values())) - conditional_entropy(list(g0.values()),pairs)\n",
    "                    results['mutual_information'] = mutual_information(pairs)\n",
    "                    # results['entropy_gn'] = entropy(list(g0.values()))\n",
    "                    results['entropy_gn'] = entropy(g0)\n",
    "                    # results['entropy_gn1'] = entropy(list(g1.values()))\n",
    "                    results['entropy_gn1'] = entropy(g1)\n",
    "                    # results['conditional_entropy'] = conditional_entropy(list(g0.values()),pairs)\n",
    "                    results['conditional_entropy'] = conditional_entropy(g0,pairs)\n",
    "                    # results['variation_gn'] = len(set(g0.values()))\n",
    "                    results['variation_gn'] = len(g0_relatives)\n",
    "                    # results['variation_gn1'] = len(set(gn1.values()))\n",
    "                    results['variation_gn1'] = len(g1_relatives)\n",
    "                    results['number_of_pairs'] = len(set(pairs))\n",
    "\n",
    "                    df.append(results)\n",
    "        \n",
    "    pd.DataFrame(df).to_csv('../data/raw/' + filename + '.csv',index=False)\n",
    "    \n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78d12a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate_MI(['Indo-European'],'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f6d71bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate_MI(families,'kinbank_mi_FINAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622ea1b",
   "metadata": {},
   "source": [
    "## Simulating kinship systems\n",
    "\n",
    "To investigate whether kinship systems have higher mutual information than chance, we can build a random baseline for each language to serve as a point of comparison.\n",
    "\n",
    "To do this, we will take each language in our dataset, and randomly scramble which terms go with which relatives (within generations). This will randomise the syncretisms within the paradigm, breaking any predictable structure built by the internal co-selection process, while maintaining the amount of variation across the system overall.\n",
    " \n",
    "We take the following steps:\n",
    "\n",
    "1. Extract the kinship system of a language from kinbank.\n",
    "2. Filter the two generations we are interested in.\n",
    "3. Randomly reassign the kinship terms to new types.\n",
    "4. Repeat the process 1000 times for each language.\n",
    "\n",
    "We already have the infrastructure for the first two! `get_kin_terms()`,  `get_pairs()` and `split_pairs()` will do this for us. So let's skip to 3, and write a function that randomises which terms form pairs, assuming that we have already extracted the kinship system and filtered the relevant pairs.\n",
    "\n",
    "### 3. Randomly rearrange kin terms\n",
    "\n",
    "`kintypes.py` also contains a list of which kin types are in which generation, which we can use to split a kinship system by generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffdacb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ks(ks):\n",
    "    gn = {}\n",
    "    gn1 = {}\n",
    "    for entry in ks:\n",
    "        if entry in kt.generation_n:\n",
    "            gn[entry] = ks[entry]\n",
    "        elif entry in kt.generation_n1:\n",
    "            gn1[entry] = ks[entry]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return gn,gn1        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ef4b2",
   "metadata": {},
   "source": [
    "Once the kinship system is split, we can then shuffle one of the generations and recombine the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0aad70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_ks(ks):\n",
    "    gn_terms = []\n",
    "    gn,gn1 = split_ks(ks)\n",
    "    \n",
    "    new_ks = {}\n",
    "    \n",
    "    for term in gn:\n",
    "        gn_terms.append(gn[term])\n",
    "\n",
    "    random.shuffle(gn_terms)\n",
    "    \n",
    "    for i in range(len(gn)):\n",
    "        # print(i)\n",
    "        key = list(gn.keys())[i]\n",
    "        new_ks[key] = gn_terms[i]\n",
    "        \n",
    "    print(entropy(list(new_ks.values())))\n",
    "\n",
    "    return {**new_ks,**gn1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea9da30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle_ks(test_ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490a175",
   "metadata": {},
   "source": [
    "If we split the kinship system, shuffle the G0 terms, and stick the system back together, then we can use `get_pairs()` just as we did for the real kinship systems to filter out the two generations we're interested in. From there, we can calculate MI for each simulation and save the data in a similar way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2677fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ks(ks: dict) -> list:\n",
    "    simulation = shuffle_ks(ks)\n",
    "    shuffled_pairs = get_pairs(simulation)\n",
    "    return shuffled_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d9b1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_MI(families: list, filename: str, times: int):\n",
    "    df = []\n",
    "    codes = []\n",
    "    filepath = '../../kinbank-family/'\n",
    "    \n",
    "    for family in tqdm(families):\n",
    "        all_files = get_kb_files(filepath + family)\n",
    "    \n",
    "        for file in all_files:\n",
    "            match = re.search(\"[a-z]{4}[0-9]{4}[a-z]?\\\\.\", file)\n",
    "            code = match.group()\n",
    "            code = code[:len(code)-1]\n",
    "            language = file.split('_' + code)[0]\n",
    "            \n",
    "            if 'Morgan' in language:\n",
    "                match = re.search(\"Morgan[0-9]{4}\",language)\n",
    "                morgan = match.group()\n",
    "                language = language.split(morgan + '_')[1]\n",
    "\n",
    "            if code not in codes:\n",
    "                codes.append(code)\n",
    "\n",
    "                ks = get_kin_terms(filepath + family + '/' + file)\n",
    "                true_pairs = get_pairs(ks)\n",
    "                \n",
    "                tg0,tg1 = split_ks(ks)\n",
    "                true_value = mutual_information(true_pairs)\n",
    "\n",
    "                for i in range(times):\n",
    "                    sim = shuffle_ks(ks)\n",
    "                    g0,g1 = split_ks(sim)\n",
    "                    pairs = get_pairs(sim)\n",
    "                    if pairs:\n",
    "                        \n",
    "                        results = {}\n",
    "                        results['language'] = language\n",
    "                        results['language_family'] = family\n",
    "                        results['code'] = code\n",
    "                        results['simulation_code'] = code + '_' + str(i)\n",
    "                        results['simulation'] = 'Y'\n",
    "                        \n",
    "                        # egn = entropy(list(g0.values()))\n",
    "                        # print(egn)\n",
    "                        # egn1 = entropy(list(g1.values()))\n",
    "                        # ce = conditional_entropy(list(g0.values()),pairs)\n",
    "                        # mi = egn1 - ce\n",
    "    \n",
    "                        results['mutual_information'] = mutual_information(pairs)\n",
    "                        results['true_value'] = true_value\n",
    "                        results['entropy_gn'] = entropy(split_pairs(pairs)[0])\n",
    "                        results['entropy_gn1'] = entropy(split_pairs(pairs)[1])\n",
    "                        results['conditional_entropy'] = conditional_entropy(split_pairs(pairs)[1],pairs)\n",
    "                        results['variation_gn'] = len(tg0)\n",
    "                        results['variation_gn1'] = len(tg1)\n",
    "                        results['number_of_pairs'] = len(set(pairs))\n",
    "#                         write_data(pairs,results)\n",
    "\n",
    "                        df.append(results)\n",
    "                \n",
    "    \n",
    "    pd.DataFrame(df).to_csv('../data/raw/' + filename + '.csv',index=False)\n",
    "    \n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191841d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simulate_MI(['Indo-European'],'test',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cab099bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_MI(families,'simulated_mi_FINAL',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19f0d1",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "\n",
    "In this section we calculate the normalised Levenshtein edit distance between pairs of kin terms in each language and generate a monte carlo sample of edit distance for each language so that we can calculate the z score (as well as the correlation between edit distances).\n",
    "\n",
    "To do this we need:\n",
    "* A function to calculate the levenshtein distance between two forms.\n",
    "* A function to calculate the distance between two meanings, which should have e.g. MB and MBD be closer than MZ and MBD.\n",
    "* A function to calculate the correlation between a table of form distances and a table of meaning distances.\n",
    "* A function to generate a monte carlo sample of correlations (ie the correlation of scrambled, simulated data).\n",
    "\n",
    "\n",
    "UPDATE: we are not just comparing all labels for all relatives, but rather all labels for all categories! This prevents languages being penalised for being compositional but not having all unique labels across aunts and uncles.\n",
    "\n",
    "So the first thing we need to do is work out which categories each language has, and who are the children of which category members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "334915f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lang = random.choice(all_kb_files)\n",
    "\n",
    "test_ks = get_kin_terms('../../kinbank/' + test_lang)\n",
    "\n",
    "gn,gn1 = split_ks(test_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f473f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_based_pairs(g1):\n",
    "    \n",
    "    # work out which g1 relatives share a term\n",
    "    \n",
    "    relatives = list(g1.keys())\n",
    "    terms = list(g1.values())\n",
    "\n",
    "    categories = []\n",
    "\n",
    "    for term in terms:\n",
    "        cat = []\n",
    "        indices = [index for index, element in enumerate(terms) if element == term]\n",
    "        for index in indices:\n",
    "            cat.append(relatives[index])\n",
    "        categories.append(cat)\n",
    "\n",
    "    categories = [list(i) for i in set(map(tuple, categories))]\n",
    "    \n",
    "    # work out which children should share a term on this basis\n",
    "    \n",
    "    child_cats = []\n",
    "\n",
    "    for cat in categories:\n",
    "        new_category = []\n",
    "        for individual in cat:\n",
    "            for pair in kt.ics_pairs:\n",
    "                if pair[0] == individual:\n",
    "                    new_category.append(pair[1])\n",
    "        child_cats.append(new_category)\n",
    "        \n",
    "    # and make a new list of parent-child pairs that should have equal semantic distance\n",
    "    \n",
    "    new_pairs = []\n",
    "    \n",
    "    for i in range(len(categories)):\n",
    "        new_pairs += list(itertools.product(categories[i], child_cats[i]))\n",
    "        \n",
    "    return new_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a4b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_based_pairs(gn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e4d1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lvs_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2254040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(g0,g1):\n",
    "    edit_dists = []\n",
    "\n",
    "    for relative1 in g0:\n",
    "        term1 = g0[relative1]\n",
    "        for relative2 in g1:\n",
    "            data = {}\n",
    "            term2 = g1[relative2]\n",
    "            \n",
    "            if relative1 == relative2:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                if len(term1) > 0 and len(term2) > 0:\n",
    "                    dist = lvs_dist(term1,term2)/len(max(term1,term2))\n",
    "                    edit_dists.append(dist)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    return edit_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dca5ab",
   "metadata": {},
   "source": [
    "To calculate semantic distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f8cfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_distance(g0,g1): # (df,ks,pairs):\n",
    "    sem_distances = []\n",
    "    \n",
    "    pairs = category_based_pairs(g1)\n",
    "\n",
    "    for relative1 in g0:\n",
    "        for relative2 in g1:\n",
    "            \n",
    "            if relative1 == relative2:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                if len(g0[relative1]) > 0 and len(g1[relative2]) > 0:\n",
    "\n",
    "                    if (relative1,relative2) in pairs or (relative2,relative1) in pairs:\n",
    "                        distance = 1\n",
    "                        sem_distances.append(distance)\n",
    "\n",
    "                    else:\n",
    "                        distance = 2\n",
    "                        sem_distances.append(distance)\n",
    "                        \n",
    "#     df['semantic_distance'] = sem_distances\n",
    "#     \n",
    "#     return df\n",
    "    return sem_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d8c4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_distance(gn,gn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0d94d",
   "metadata": {},
   "source": [
    "The above will get us a meaning distance measure where if you are the child of a person, your meaning distance will be 1, and if you are not, your distance will be 2.\n",
    "\n",
    "This captures the kind of compositionality in meaning we're interested in (e.g. that children of a person will have a similar form to that person) but not all kinds of kinship compositionality e.g. the Swedish situation.\n",
    "\n",
    "Might have to do something more complex with features, e.g.\n",
    "male vs female\n",
    "mother side vs father side\n",
    "young vs old\n",
    "child-of vs not child-of\n",
    "\n",
    "Anyway, for now we can calculate the correlation between our edit distance and our semantic distance with scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa32731",
   "metadata": {},
   "source": [
    "But to get a z-score, we can just reuse our shuffling infrastructure from the original simulation (repeated here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf728cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_ks(ks):\n",
    "    gn_terms = []\n",
    "    gn,gn1 = split_ks(ks)\n",
    "    for term in gn:\n",
    "        gn_terms.append(gn[term])\n",
    "\n",
    "    random.shuffle(gn_terms)\n",
    "    \n",
    "    for i in range(len(gn)):\n",
    "        # print(i)\n",
    "        key = list(gn.keys())[i]\n",
    "        ks[key] = gn_terms[i]\n",
    "\n",
    "    return ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3169b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shuffle_distance(distances):\n",
    "#     shuffled_edit_dists = distances.copy()\n",
    "#     random.shuffle(shuffled_edit_dists)\n",
    "#     return shuffled_edit_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df88d4",
   "metadata": {},
   "source": [
    "And finally combine all of that into a single function that calculates the true correlation between edit distance and semantic distance for each language; simulates that language 1000 times and calculates the same correlation; and saves a z-score for each language.\n",
    "\n",
    "**EDIT: as of 25/06 we are no longer looking at semantic distance - instead, we are calculating the average edit distance across all parent-child pairs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0601234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distance(families,times,filename):\n",
    "    output_df = []\n",
    "    \n",
    "    codes = []\n",
    "    \n",
    "    for family in tqdm(families):\n",
    "        files = get_kb_files('../../kinbank-family/' + family)\n",
    "        for file in files:\n",
    "            \n",
    "            match = re.search('[a-z]{4}[0-9]{4}[a-z]?\\\\.', file)\n",
    "            code = match.group()\n",
    "            code = code.split('.')[0]\n",
    "            language = file.split('_' + code)[0]\n",
    "\n",
    "            if 'Morgan' in language:\n",
    "                match = re.search(\"Morgan[0-9]{4}\",language)\n",
    "                morgan = match.group()\n",
    "                language = language.split(morgan + '_')[1]\n",
    "                \n",
    "            data = {}\n",
    "\n",
    "            if code not in codes:\n",
    "                codes.append(code)\n",
    "\n",
    "                full_ks = get_kin_terms('../../kinbank/' + file)\n",
    "                gn,gn1 = split_ks(full_ks)\n",
    "#                 ks = {**gn,**gn1}\n",
    "                \n",
    "\n",
    "                edit = edit_distance(gn,gn1)\n",
    "                sem = semantic_distance(gn,gn1)\n",
    "        \n",
    "                if len(edit) > 2:\n",
    "                    true_corr = scipy.stats.pearsonr(np.array(edit),np.array(sem))[0]\n",
    "\n",
    "                    sample_correlations = []\n",
    "                    for i in range(times):\n",
    "                        sample = shuffle_ks(full_ks)\n",
    "                        sample_gn,sample_gn1 = split_ks(sample)\n",
    "                        sample_edit = edit_distance(sample_gn,sample_gn1)\n",
    "                        sample_sem = semantic_distance(sample_gn,sample_gn1)\n",
    "                        sample_corr = scipy.stats.pearsonr(np.array(sample_edit),np.array(sample_sem))[0]\n",
    "                        sample_correlations.append(sample_corr)\n",
    "\n",
    "#                     print(sample_correlations)\n",
    "                    mean = np.mean(sample_correlations)\n",
    "                    print(mean)\n",
    "                    sd = np.std(sample_correlations)\n",
    "                    print(sd)\n",
    "                    z = (true_corr - mean) / sd\n",
    "                    print(z)\n",
    "\n",
    "                    data['language'] = language\n",
    "                    data['family'] = family\n",
    "                    data['code'] = code\n",
    "                    data['correlation'] = true_corr\n",
    "                    data['mean'] = mean\n",
    "                    data['sd'] = sd\n",
    "                    data['z'] = z\n",
    "\n",
    "                    output_df.append(data)\n",
    "                \n",
    "    pd.DataFrame(output_df).to_csv('../data/raw/' + filename + '.csv',index=False)\n",
    "\n",
    "    return pd.DataFrame(output_df)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08200e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distance(families,times,filename):\n",
    "    output_df = []\n",
    "    \n",
    "    codes = []\n",
    "        \n",
    "    for family in tqdm(families):\n",
    "        \n",
    "        files = get_kb_files('../../kinbank-family/' + family)\n",
    "        \n",
    "        for file in files:\n",
    "\n",
    "            print(file)\n",
    "                \n",
    "            match = re.search('[a-z]{4}[0-9]{4}[a-z]?\\\\.', file)\n",
    "            code = match.group()\n",
    "            code = code.split('.')[0]\n",
    "            language = file.split('_' + code)[0]\n",
    "\n",
    "            if 'Morgan' in language:\n",
    "                match = re.search(\"Morgan[0-9]{4}\",language)\n",
    "                morgan = match.group()\n",
    "                language = language.split(morgan + '_')[1]\n",
    "                                \n",
    "            data = {}\n",
    "\n",
    "            if code not in codes:\n",
    "                codes.append(code)\n",
    "\n",
    "                full_ks = get_kin_terms('../../kinbank/' + file)\n",
    "                pairs = get_pairs(full_ks)\n",
    "                \n",
    "                edit_distance = []\n",
    "                for pair in pairs:\n",
    "                    try:\n",
    "                        ed = lvs_dist(pair[0],pair[1]) / max(len(pair[0]),len(pair[1]))\n",
    "                        edit_distance.append(ed)\n",
    "                    except:\n",
    "                        continue\n",
    "                try:            \n",
    "                    true_mean = statistics.mean(edit_distance)\n",
    "                except:\n",
    "                    continue\n",
    "#                 print(true_mean)\n",
    "\n",
    "                sample_avgs = []\n",
    "                for i in tqdm(range(times)):\n",
    "                    sample = shuffle_ks(full_ks)\n",
    "                    sample_pairs = get_pairs(sample)\n",
    "                    distances = []\n",
    "                    for pair in sample_pairs:\n",
    "                        try:\n",
    "                            sample_ed = lvs_dist(pair[0],pair[1]) / max(len(pair[0]),len(pair[1]))\n",
    "                            distances.append(sample_ed)\n",
    "                        except:\n",
    "                            continue\n",
    "                    sample_avg = statistics.mean(distances)\n",
    "                    sample_avgs.append(sample_avg)\n",
    "\n",
    "#                 print(sample_avgs)\n",
    "\n",
    "# #                     print(sample_correlations)\n",
    "                sample_mean = statistics.mean(sample_avgs)\n",
    "#                 print(sample_mean)\n",
    "                sd = statistics.stdev(sample_avgs)\n",
    "#                 print(sd)\n",
    "                if true_mean == sample_mean:\n",
    "                    z = 0\n",
    "                else:\n",
    "                    z = (true_mean - sample_mean) / sd\n",
    "\n",
    "#                 print(z)\n",
    "\n",
    "                data['language'] = language\n",
    "                data['family'] = family\n",
    "                data['code'] = code\n",
    "                data['average_edit_distance'] = true_mean\n",
    "                data['sample_mean'] = sample_mean\n",
    "                data['sd'] = sd\n",
    "                data['z'] = -z\n",
    "\n",
    "                output_df.append(data)\n",
    "                \n",
    "    pd.DataFrame(output_df).to_csv('../data/raw/' + filename + '.csv',index=False)\n",
    "\n",
    "    return pd.DataFrame(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d0ce50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_distance(['Indo-European'],100,'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6952b8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_distance(families, 1000, 'average_edit_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053b99e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fDD': 'dhevtī', 'meZ': 'dīdī', 'mF': 'bāp', 'mM': 'mātā', 'mSS': 'potā', 'mSD': 'potī', 'mDS': 'dhevtā', 'mFB': 'tāū', 'mFZ': 'buā', 'mFyB': 'cācā', 'mFeB': 'tāū', 'mFeZ': 'buā', 'mFyZ': 'buā', 'mBS': 'bhatījā', 'mBD': 'bhatījī', 'mZS': 'bhānjā', 'mZD': 'bhānjī', 'meBS': 'bhatījā', 'myBS': 'bhatījā', 'meBD': 'bhatījī', 'myBD': 'bhatījī', 'meZS': 'bhānjā', 'myZS': 'bhānjā', 'meZD': 'bhānjī', 'myZD': 'bhānjī', 'mBW': 'bhābahū', 'mZH': 'bahenoī', 'mWB': 'sālā', 'mHB': 'devar', 'mSW': 'bahū', 'mDH': 'jamāī', 'feZ': 'dīdī', 'fF': 'bāp', 'fM': 'mātā', 'fSS': 'potā', 'fSD': 'potī', 'fDS': 'dhevtā', 'fFB': 'tāū', 'fFZ': 'buā', 'fFyB': 'cācā', 'fFeB': 'tāū', 'fFeZ': 'buā', 'fFyZ': 'buā', 'fBS': 'bhatījā', 'fBD': 'bhatījī', 'fZS': 'bhānjā', 'fZD': 'bhānjī', 'feBS': 'bhatījā', 'fyBS': 'bhatījā', 'feBD': 'bhatījī', 'fyBD': 'bhatījī', 'feZS': 'bhānjā', 'fyZS': 'bhānjā', 'feZD': 'bhānjī', 'fyZD': 'bhānjī', 'fBW': 'bhābahū', 'fZH': 'bahenoī', 'fWB': 'sālā', 'fHB': 'devar', 'fSW': 'bahū', 'fDH': 'jamāī', 'mH': 'pati', 'mW': 'bahū', 'fH': 'pati', 'fW': 'bahū', 'mB': 'bhāī', 'mZ': 'bahan', 'mS': 'beṭā', 'mD': 'beṭī', 'mFF': 'dādā', 'mFM': 'dādī', 'mMM': 'nānī', 'mDD': 'dhevtī', 'mMB': 'māmā', 'mMZ': 'mausī', 'mMeZ': 'mausī', 'mMyZ': 'mausī', 'mMeB': 'māmā', 'mMyB': 'māmā', 'mFZD': 'phupheri bahan', 'mFBD': 'caceri bahan', 'mMBD': 'mameri bahan', 'mMZD': 'mɔseri bahan', 'mFBS': 'cacera bhai', 'mFZS': 'phuphera bhai', 'mMBS': 'mamera bhai', 'mFeBS': 'cacera bhai', 'mFyBS': 'cacera bhai', 'mFeZS': 'phuphera bhai', 'mFyZS': 'phuphera bhai', 'mFeBD': 'caceri bahan', 'mFyBD': 'caceri bahan', 'mFeZD': 'phupheri bahan', 'mFyZD': 'phupheri bahan', 'mMeBS': 'mamera bhai', 'mMyBS': 'mamera bhai', 'mMeBD': 'mameri bahan', 'mMyBD': 'mameri bahan', 'mMeZD': 'mɔseri bahan', 'mMyZD': 'mɔseri bahan', 'mFBeS': 'cacera bhai', 'mFByS': 'cacera bhai', 'mFZeS': 'phuphera bhai', 'mFZyS': 'phuphera bhai', 'mFBeD': 'caceri bahan', 'mFByD': 'caceri bahan', 'mFZeD': 'phupheri bahan', 'mFZyD': 'phupheri bahan', 'mMBeS': 'mamera bhai', 'mMByS': 'mamera bhai', 'mMBeD': 'mameri bahan', 'mMByD': 'mameri bahan', 'mMZeD': 'mɔseri bahan', 'mMZyD': 'mɔseri bahan', 'mHF': 'sasur', 'mHM': 'sās', 'mWF': 'sasur', 'mWM': 'sās', 'mWZ': 'sālī', 'mHZ': 'nanad', 'fB': 'bhāī', 'fZ': 'bahan', 'fS': 'beṭā', 'fD': 'beṭī', 'fFF': 'dādā', 'fFM': 'dādī', 'fMM': 'nānī', 'fMB': 'māmā', 'fMZ': 'mausī', 'fMeZ': 'mausī', 'fMyZ': 'mausī', 'fMeB': 'māmā', 'fMyB': 'māmā', 'fFZD': 'phupheri bahan', 'fFBD': 'caceri bahan', 'fMBD': 'mameri bahan', 'fMZD': 'mɔseri bahan', 'fFBS': 'cacera bhai', 'fFZS': 'phuphera bhai', 'fMBS': 'mamera bhai', 'fFeBS': 'cacera bhai', 'fFyBS': 'cacera bhai', 'fFeZS': 'phuphera bhai', 'fFyZS': 'phuphera bhai', 'fFeBD': 'caceri bahan', 'fFyBD': 'caceri bahan', 'fFeZD': 'phupheri bahan', 'fFyZD': 'phupheri bahan', 'fMeBS': 'mamera bhai', 'fMyBS': 'mamera bhai', 'fMeBD': 'mameri bahan', 'fMyBD': 'mameri bahan', 'fMeZD': 'mɔseri bahan', 'fMyZD': 'mɔseri bahan', 'fFBeS': 'cacera bhai', 'fFByS': 'cacera bhai', 'fFZeS': 'phuphera bhai', 'fFZyS': 'phuphera bhai', 'fFBeD': 'caceri bahan', 'fFByD': 'caceri bahan', 'fFZeD': 'phupheri bahan', 'fFZyD': 'phupheri bahan', 'fMBeS': 'mamera bhai', 'fMByS': 'mamera bhai', 'fMBeD': 'mameri bahan', 'fMByD': 'mameri bahan', 'fMZeD': 'mɔseri bahan', 'fMZyD': 'mɔseri bahan', 'fHF': 'sasur', 'fHM': 'sās', 'fWF': 'sasur', 'fWM': 'sās', 'fWZ': 'sālī', 'fHZ': 'nanad', 'meB': 'baṛa bhai', 'myZ': 'choṭī bhan', 'mA': 'purvaj', 'mMF': 'nānā', 'mSWM': 'sambandhi', 'mSWF': 'sambandhi', 'mFZH': 'fufa-ji', 'mFBW': 'chahu', 'mMZH': 'maasu', 'mMBW': 'mami', 'feB': 'baṛa bhai', 'fyZ': 'choṭī bhan', 'fA': 'purvaj', 'fMF': 'nānā', 'fSWM': 'sambandhi', 'fSWF': 'sambandhi', 'fFZH': 'fufa-ji', 'fFBW': 'chachu', 'fMZH': 'maasu', 'fMBW': 'mami', 'mMZS': 'mɔsera bhai', 'mMeZS': 'mɔsera bhai', 'mMyZS': 'mɔsera bhai', 'mMZeS': 'mɔsera bhai', 'mMZyS': 'mɔsera bhai', 'fMZS': 'mɔsera bhai', 'fMeZS': 'mɔsera bhai', 'fMyZS': 'mɔsera bhai', 'fMZeS': 'mɔsera bhai', 'fMZyS': 'mɔsera bhai', 'myB': 'choṭā bhāī', 'fyB': 'choṭā bhāī'}\n",
      "[('mātā', 'baṛa bhai'), ('mātā', 'choṭā bhāī'), ('mātā', 'dīdī'), ('mātā', 'choṭī bhan'), ('mātā', 'baṛa bhai'), ('mātā', 'choṭā bhāī'), ('mātā', 'dīdī'), ('mātā', 'choṭī bhan'), ('māmā', 'mamera bhai'), ('māmā', 'mameri bahan'), ('māmā', 'mamera bhai'), ('māmā', 'mameri bahan'), ('māmā', 'mamera bhai'), ('māmā', 'mameri bahan'), ('māmā', 'mamera bhai'), ('māmā', 'mameri bahan'), ('mausī', 'mɔsera bhai'), ('mausī', 'mɔseri bahan'), ('mausī', 'mɔsera bhai'), ('mausī', 'mɔseri bahan'), ('mausī', 'mɔsera bhai'), ('mausī', 'mɔseri bahan'), ('mausī', 'mɔsera bhai'), ('mausī', 'mɔseri bahan'), ('tāū', 'cacera bhai'), ('tāū', 'caceri bahan'), ('cācā', 'cacera bhai'), ('cācā', 'caceri bahan'), ('tāū', 'cacera bhai'), ('tāū', 'caceri bahan'), ('cācā', 'cacera bhai'), ('cācā', 'caceri bahan'), ('buā', 'phuphera bhai'), ('buā', 'phupheri bahan'), ('buā', 'phuphera bhai'), ('buā', 'phupheri bahan'), ('buā', 'phuphera bhai'), ('buā', 'phupheri bahan'), ('buā', 'phuphera bhai'), ('buā', 'phupheri bahan')]\n"
     ]
    }
   ],
   "source": [
    "cornish = get_kin_terms('../../kinbank/Cornish_corn1251.csv')\n",
    "bende = get_kin_terms('../../kinbank/Bende_bend1258.csv')\n",
    "tagalog = get_kin_terms('../../kinbank/Tagalog_taga1270.csv')\n",
    "andi = get_kin_terms('../../kinbank/Andi_andi1255.csv')\n",
    "hindi = get_kin_terms('../../kinbank/Hindi_hind1269.csv')\n",
    "\n",
    "cornish_pairs = get_pairs(get_kin_terms('../../kinbank/Cornish_corn1251.csv'))\n",
    "\n",
    "english_pairs = get_pairs(get_kin_terms('../../kinbank/English_stan1293.csv'))\n",
    "\n",
    "bende_pairs = get_pairs(bende)\n",
    "\n",
    "tagalog_pairs = get_pairs(tagalog)\n",
    "\n",
    "andi_pairs = get_pairs(andi)\n",
    "\n",
    "hindi_pairs = get_pairs(hindi)\n",
    "\n",
    "print(hindi)\n",
    "print(hindi_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "419723d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888043623043623"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = []\n",
    "for pair in hindi_pairs:\n",
    "    dists.append(lvs_dist(pair[0],pair[1]) / max(len(pair[0]),len(pair[1])))\n",
    "\n",
    "statistics.mean(dists)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c69bee30-7aed-474e-8676-a81c40c5ee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9290237512487513"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_avgs = []\n",
    "\n",
    "for i in range(50):\n",
    "    sample = shuffle_ks(hindi)\n",
    "    sample_pairs = get_pairs(sample)\n",
    "    distances = []\n",
    "    for pair in sample_pairs:\n",
    "        try:\n",
    "            sample_ed = lvs_dist(pair[0],pair[1]) / max(len(pair[0]),len(pair[1]))\n",
    "            distances.append(sample_ed)\n",
    "        except:\n",
    "            continue\n",
    "    sample_avg = statistics.mean(distances)\n",
    "    sample_avgs.append(sample_avg)\n",
    "\n",
    "statistics.mean(sample_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c82f9-8725-408f-a28e-7fce3b48321f",
   "metadata": {},
   "source": [
    "ottowa_ojibwa otta1242\n",
    "yurok yuro 1248\n",
    "wapishana wapi 1253\n",
    "mokpwe mokp1239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ddb3487-873a-4fb7-b83d-1ca734f2f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pairs(ks: dict) -> list:\n",
    "#     pairs_of_terms = []\n",
    "#     parent_types = []\n",
    "\n",
    "#     for pair in kt.pairs_no_parents:\n",
    "#         if pair[0] in ks and pair[1] in ks:\n",
    "#             pairs_of_terms.append((ks[pair[0]],ks[pair[1]]))\n",
    "#             parent_types.append(pair[0])\n",
    "                \n",
    "#     return pairs_of_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b995bc04-793b-46f6-a668-3130a7d5ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mMeBS': 'mwanyango', 'mMyBS': 'mwanyango', 'mMeZS': 'mwanyango', 'mMyZS': 'mwanyango', 'mMeBD': 'ndomé', 'mMyBD': 'ndomé', 'mMeZD': 'ndomé', 'mMyZD': 'ndomé', 'fMeBS': 'ndomé', 'fMyBS': 'ndomé', 'fMeZS': 'ndomé', 'fMyZS': 'ndomé', 'fMeBD': 'mwanyango', 'fMyBD': 'mwanyango', 'fMeZD': 'mwanyango', 'fMyZD': 'mwanyango', 'mFeBS': 'mwanyango', 'mFyBS': 'mwanyango', 'mFeZS': 'mwanyango', 'mFyZS': 'mwanyango', 'mFeBD': 'ndomé', 'mFyBD': 'ndomé', 'mFeZD': 'ndomé', 'mFyZD': 'ndomé', 'fFeBS': 'ndomé', 'fFyBS': 'ndomé', 'fFeZS': 'ndomé', 'fFyZS': 'ndomé', 'fFeBD': 'mwanyango', 'fFyBD': 'mwanyango', 'fFeZD': 'mwanyango', 'fFyZD': 'mwanyango', 'meB': 'waólúlu', 'myB': 'wenongoni', 'meZ': 'waólúlu', 'myZ': 'wenongoni', 'feB': 'waólúlu', 'fyB': 'wenongoni', 'feZ': 'waólúlu', 'fyZ': 'wenongoni'} \n",
      "\n",
      " {'mFeB': 'melá', 'mFyB': 'melá', 'mFeZ': 'atí', 'mFyZ': 'atí', 'mMeZ': 'atí', 'mMyZ': 'atí', 'fFeB': 'melá', 'fFyB': 'melá', 'fFeZ': 'atí', 'fFyZ': 'atí', 'fMeZ': 'atí', 'fMyZ': 'atí', 'mF': 'rángó', 'mM': 'jéjé', 'mMeB': 'móla', 'mMyB': 'móla', 'fF': 'rángó', 'fM': 'jéjé', 'fMeB': 'móla', 'fMyB': 'móla'} \n",
      "\n",
      "[('móla', 'mwanyango'), ('móla', 'ndomé'), ('móla', 'mwanyango'), ('móla', 'ndomé'), ('móla', 'ndomé'), ('móla', 'mwanyango'), ('móla', 'ndomé'), ('móla', 'mwanyango'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'ndomé'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'mwanyango'), ('melá', 'mwanyango'), ('melá', 'ndomé'), ('melá', 'mwanyango'), ('melá', 'ndomé'), ('melá', 'ndomé'), ('melá', 'mwanyango'), ('melá', 'ndomé'), ('melá', 'mwanyango'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'ndomé'), ('atí', 'mwanyango'), ('atí', 'ndomé'), ('atí', 'mwanyango')]\n"
     ]
    }
   ],
   "source": [
    "ojibwa = get_kin_terms('../../kinbank/Ottowa_Ojibwa_otta1242.csv') # missing cell values, need to remove empty strings\n",
    "yurok = get_kin_terms('../../kinbank/Yurok_yuro1248.csv') # nuncles and cousins split by gender, siblings split by age + gender\n",
    "wapishana = get_kin_terms('../../kinbank/Wapishana_wapi1253.csv') # older siblings split by age and gender, younger not split by gender. cousins all the same. nuncles split by gender and side of the family\n",
    "mokpwe = get_kin_terms('../../kinbank/Mokpwe_mokp1239.csv') # opposite gender cousins split by gender, siblings split by age, nuncles split by gender and mother's brother distinguished\n",
    "\n",
    "print(split_ks(mokpwe)[0], '\\n\\n', split_ks(mokpwe)[1], '\\n')\n",
    "print(get_pairs(mokpwe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4978fe1e-ddaf-4a8c-a4ad-7bfc57924ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72193"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_information(get_pairs(mokpwe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fa90d-ab2a-4eeb-8745-20b2f182a909",
   "metadata": {},
   "source": [
    "Lots of languages get an MI score of 0.9183 because that's what you get if your cousins and nuncles have an MI of 0, but your parents and siblings are distinct from them. This is not a problem! It is still interesting from a structural point of view - namely, that languages seek to distinguish parents and siblings from cousins and nuncles because you can specify people more informatively that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3f17f-a7d0-4f26-a6be-cc46691ece18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
